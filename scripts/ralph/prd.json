{
  "project": "CoinTradingBot-V6-AdvancedTrading",
  "branchName": "ralph/v6-advanced-trading",
  "description": "V6 고급 트레이딩: 실제 시장 데이터 기반 리서치, GARCH/VaR 리스크 모델, 프론트엔드 분석 강화, 메트릭 영속화, 시장 레짐 감지, Docker 컨테이너화. V5까지 완성된 2202+ 테스트와 완전한 하위 호환을 유지하면서 프로덕션급 트레이딩 시스템으로 진화시킨다.",
  "userStories": [
    {
      "id": "V6-001",
      "title": "HistoricalDataProvider — 리서치용 실제 데이터 공급 모듈",
      "description": "현재 리서치 실험들은 합성 데이터(numpy random)만 사용한다. DataStore에 이미 OHLCV, 펀딩비 데이터가 저장되어 있으므로, 이를 리서치 프레임워크에 연결하는 데이터 공급 모듈을 만든다. 실험들이 실제 시장 데이터로 백테스트하면 파라미터 최적화의 신뢰도가 크게 올라간다.",
      "acceptanceCriteria": [
        "Create src/bot/research/data_provider.py with HistoricalDataProvider class",
        "Constructor: __init__(self, data_store: DataStore) — DataStore 인스턴스를 주입받음",
        "Method get_prices(symbol, timeframe='1h', lookback_days=30) -> list[float]: DataStore.get_candles()에서 close 가격 리스트를 추출하여 반환. 시간순 정렬 (오래된→최신). 데이터 없으면 빈 리스트 반환",
        "Method get_ohlcv(symbol, timeframe='1h', lookback_days=30) -> list[OHLCV]: 원본 OHLCV 모델 리스트 반환 (ATR 계산 등에 필요)",
        "Method get_returns(symbol, timeframe='1h', lookback_days=30) -> list[float]: close-to-close 수익률 시계열 반환. len(returns) == len(prices) - 1",
        "Method get_funding_rates(symbol, lookback_days=30) -> list[dict]: DataStore.get_funding_rates()에서 [{timestamp, funding_rate, mark_price, spot_price}] 형태로 반환",
        "Method get_multi_prices(symbols: list[str], timeframe='1h', lookback_days=30) -> dict[str, list[float]]: 여러 심볼의 가격을 한번에 가져옴. 내부적으로 asyncio.gather로 병렬 조회",
        "Method get_available_symbols(timeframe='1h', min_candles=100) -> list[str]: DataStore에 충분한 데이터가 있는 심볼 목록 반환. DataStore에 get_available_symbols() 쿼리 추가 필요 (SELECT DISTINCT symbol FROM ohlcv WHERE timeframe=? GROUP BY symbol HAVING COUNT(*) >= ?)",
        "DataStore에 get_available_symbols(timeframe, min_count) async 메서드 추가: DISTINCT symbol 쿼리",
        "HistoricalDataProvider는 async 메서드들을 가지며, 리서치 실험에서 사용 시 asyncio.run() 또는 실험 내부에서 await로 호출",
        "Add HistoricalDataProvider to src/bot/research/__init__.py exports",
        "Create tests/research/test_data_provider.py: mock DataStore로 get_prices, get_returns, get_funding_rates, get_multi_prices, get_available_symbols 테스트. 빈 데이터 처리, lookback 필터링 검증",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "HistoricalDataProvider는 DataStore의 thin wrapper다. 복잡한 로직 넣지 말 것. get_multi_prices에서 asyncio.gather 사용. get_candles의 limit 파라미터를 lookback_days에 맞게 계산 (1h → 24*days, 4h → 6*days 등)."
    },
    {
      "id": "V6-002",
      "title": "DataCollector 배치 백필 + 스캐너 연동 자동 수집",
      "description": "리서치가 실제 데이터를 쓰려면 데이터가 충분히 쌓여야 한다. DataCollector에 배치 백필(bulk backfill) 기능을 추가하고, TokenScanner가 발견한 심볼을 자동으로 수집 대상에 포함시킨다. 이를 통해 스캐너가 발견한 기회 심볼의 히스토리컬 데이터가 자동으로 축적된다.",
      "acceptanceCriteria": [
        "Add bulk_backfill(symbols, timeframe='1h', days=30) async method to DataCollector: 여러 심볼의 과거 데이터를 배치로 가져옴",
        "bulk_backfill 내부: 거래소 API rate limit 존중 (심볼 간 0.5초 딜레이), 진행률 로깅 (structlog: symbol X/N 완료), 이미 있는 데이터는 skip (DataStore의 INSERT OR IGNORE 활용)",
        "Add auto_discover_symbols(registry: OpportunityRegistry, min_score=30.0) method to DataCollector: OpportunityRegistry에서 상위 심볼 가져와서 self._symbols에 동적 추가",
        "DataCollector에 _dynamic_symbols: set[str] 필드 추가. auto_discover에서 발견한 심볼 저장. collect_once()에서 self._symbols + self._dynamic_symbols 합쳐서 수집",
        "Add _backfill_loop() async method: EngineManager에서 background task로 실행 가능. 매 6시간마다 dynamic_symbols의 백필 실행 (최근 7일 데이터 확인 및 보충)",
        "Config에 추가: data_backfill_enabled: bool = True, data_backfill_interval_hours: float = 6.0, data_backfill_days: int = 30",
        "SETTINGS_METADATA에 위 3개 설정 추가 (section: 'Data Collection')",
        "EngineManager.start_background_loops()에서 data_backfill_enabled일 때 DataCollector._backfill_loop() 시작",
        "main.py _init_engine_mode()에서 DataCollector 생성 시 OpportunityRegistry 전달하여 auto_discover 연결",
        "Create tests/data/test_backfill.py: mock exchange로 bulk_backfill 테스트 (rate limit delay 검증은 시간 mock으로), auto_discover_symbols 테스트, _dynamic_symbols 합산 검증",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Rate limiting이 핵심. ccxt exchange.fetchOHLCV는 보통 500-1000개 캔들 반환. 30일 1h = 720캔들이므로 1회 호출로 충분. 4h면 180캔들. bulk_backfill에서 각 심볼별로 1회 API 콜이면 충분하고, 0.5초 간격이면 20개 심볼 = 10초."
    },
    {
      "id": "V6-003",
      "title": "리서치 실험 실제 데이터 업그레이드 — 4개 실험 모두 리얼 데이터 사용",
      "description": "기존 4개 리서치 실험(volatility_regime, cointegration, optimal_grid, funding_prediction)이 HistoricalDataProvider를 통해 실제 거래소 데이터를 사용하도록 업그레이드한다. 실제 데이터가 없으면 기존 합성 데이터로 fallback하여 하위 호환 유지.",
      "acceptanceCriteria": [
        "ResearchTask ABC에 optional data_provider: HistoricalDataProvider | None = None 파라미터 추가. __init__(self, data_provider=None)으로 base class에서 저장. 기존 서브클래스 하위 호환 유지 (default None)",
        "VolatilityRegimeExperiment.run_experiment(): data_provider가 있으면 get_prices('BTC/USDT', '1h', lookback_days=60)로 실제 데이터 가져옴. 없거나 데이터 부족하면 기존 _generate_synthetic_prices() 사용. results에 'data_source': 'real' | 'synthetic' 필드 추가",
        "CointegrationExperiment.run_experiment(): data_provider가 있으면 stat_arb_pairs 설정에서 페어 읽어서 get_multi_prices()로 실제 데이터 가져옴. Engle-Granger 테스트를 실제 가격 페어에 대해 실행. 결과에 p-value, cointegration strength 포함",
        "OptimalGridExperiment.run_experiment(): data_provider가 있으면 grid_symbols의 첫 번째 심볼에 대해 실제 1h 가격 가져옴. 다양한 grid_spacing(0.2%, 0.5%, 1.0%, 1.5%, 2.0%) × grid_levels(5, 10, 15, 20) 조합으로 SimpleBacktestRunner 실행. 최적 파라미터 추천",
        "FundingPredictionExperiment.run_experiment(): data_provider가 있으면 get_funding_rates('BTC/USDT', lookback_days=30)로 실제 펀딩비 히스토리 가져옴. 시간대별(0,8,16시) 평균 펀딩비 패턴 분석. 요일별 패턴 분석. 결과에 best_entry_hour, avg_positive_rate 포함",
        "각 실험의 run_experiment()에 kwargs로 실제 데이터를 직접 전달하는 기존 방식도 유지 (kwargs['prices']가 있으면 그것 사용 > data_provider > synthetic 순서)",
        "research/experiments/__init__.py: 모든 실험 클래스 export 유지",
        "EngineManager._research_loop()에서 실험 생성 시 data_provider 전달 (DataStore가 있을 때)",
        "Create tests/research/test_experiments_real_data.py: mock HistoricalDataProvider로 각 실험이 실제 데이터 모드에서 동작하는지 테스트. fallback to synthetic 테스트. data_source 필드 검증",
        "기존 tests/research/ 테스트 모두 통과 (합성 데이터 모드 유지)",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "핵심: data_provider=None이면 기존과 100% 동일하게 동작해야 함. 각 실험은 async run_experiment()로 바꿔야 할 수 있음 — DataProvider가 async이므로. 또는 run_experiment() 내부에서 asyncio.get_event_loop().run_until_complete() 사용. ResearchTask ABC의 run_experiment는 sync로 유지하되, 내부에서 async 호출을 sync로 래핑하는 helper 사용."
    },
    {
      "id": "V6-004",
      "title": "리서치 자동 배포 파이프라인 — A/B 검증 + 안전한 파라미터 적용",
      "description": "리서치 실험에서 유의미한 결과가 나왔을 때, 자동으로 파라미터를 적용하되 A/B 비교 검증과 안전장치를 거치도록 한다. 잘못된 파라미터 적용 시 자동 롤백으로 리스크를 최소화한다.",
      "acceptanceCriteria": [
        "Create src/bot/research/deployer.py with ResearchDeployer class",
        "ResearchDeployer.__init__(self, tuner: ParameterTuner, settings: Settings, tracker: EngineTracker)",
        "Method evaluate_report(report: ResearchReport) -> DeployDecision: improvement_significant=True이고 recommended_changes가 있을 때만 DEPLOY 결정. 그 외 SKIP. DeployDecision dataclass: action ('deploy'|'skip'|'rollback'), reason, changes",
        "Method deploy(report: ResearchReport) -> DeployResult: (1) 현재 파라미터 스냅샷 저장 (rollback용), (2) tuner.apply_changes()로 적용, (3) deploy_history에 기록. DeployResult dataclass: success, deployed_changes, snapshot_id, timestamp",
        "Method check_regression(engine_name: str, hours_since_deploy=24) -> bool: deploy 후 tracker.get_metrics()의 sharpe가 deploy 전 sharpe 대비 30% 이상 하락하면 True (regression detected)",
        "Method rollback(snapshot_id: str) -> bool: 저장된 파라미터 스냅샷으로 settings.reload() 실행. 롤백 이력 기록",
        "Method get_deploy_history() -> list[dict]: 배포 이력 반환 (timestamp, report_name, changes, rolled_back)",
        "_param_snapshots: dict[str, dict] — snapshot_id → {params dict} 저장",
        "_deploy_history: list[DeployRecord] — 배포 이력",
        "Safety bounds: 단일 배포에서 변경 가능한 파라미터 수 max 3개, 각 파라미터 변경폭 TUNER_CONFIG bounds 내로 제한",
        "EngineManager._research_loop()에서 실험 완료 후 deployer.evaluate_report() → deploy() 호출 추가",
        "EngineManager에 _regression_check_loop() 추가: 매 6시간마다 최근 deploy 건에 대해 check_regression() 실행, regression이면 auto rollback",
        "Config: research_auto_deploy: bool = True, research_regression_check_hours: float = 6.0",
        "SETTINGS_METADATA에 위 2개 설정 추가",
        "GET /api/research/deployments 엔드포인트 추가: 배포 이력 + 롤백 이력",
        "Create tests/research/test_deployer.py: evaluate_report (significant/not), deploy + snapshot, check_regression (positive/negative), rollback, safety bounds, deploy history",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "안전이 최우선. deploy 후 반드시 pre-deploy snapshot을 저장. rollback은 항상 가능해야 함. regression threshold는 보수적으로 (30% 하락). auto_deploy=False면 report만 저장하고 deploy하지 않음."
    },
    {
      "id": "V6-005",
      "title": "VolatilityService — GARCH 기반 실시간 변동성 예측 서비스",
      "description": "기존 GARCHModel(quant/volatility.py)과 classify_volatility_regime()을 래핑하여, 심볼별로 주기적으로 GARCH를 피팅하고 변동성 예측값과 레짐 분류를 제공하는 서비스 모듈을 만든다. 이후 리스크 모델과 엔진에서 이 서비스를 참조한다.",
      "acceptanceCriteria": [
        "Create src/bot/risk/volatility_service.py with VolatilityService class",
        "VolatilityService.__init__(self, data_provider: HistoricalDataProvider | None = None): 내부에 _models: dict[str, GARCHModel], _forecasts: dict[str, float], _regimes: dict[str, VolatilityRegime], _last_fit: dict[str, datetime] 저장",
        "Method async fit_symbol(symbol: str, lookback_days=60): data_provider에서 get_returns() 가져와서 GARCHModel.fit() 실행. 결과를 _models[symbol], _forecasts[symbol]에 저장. _regimes[symbol]은 classify_volatility_regime()으로 분류",
        "Method async fit_all(symbols: list[str]): 모든 심볼에 대해 fit_symbol() 실행. 실패한 심볼은 skip하고 로깅",
        "Method get_forecast(symbol: str, horizon=1) -> float | None: 캐시된 변동성 예측값 반환. fit 안 됐으면 None",
        "Method get_regime(symbol: str) -> VolatilityRegime: 캐시된 레짐 반환. 없으면 NORMAL",
        "Method get_all_regimes() -> dict[str, VolatilityRegime]: 전체 레짐 맵 반환",
        "Method get_market_regime() -> VolatilityRegime: BTC/USDT의 레짐을 시장 전체 레짐 대리변수로 사용. BTC 레짐 없으면 NORMAL",
        "Method needs_refit(symbol: str, max_age_hours=6.0) -> bool: 마지막 fit 시점이 max_age_hours보다 오래됐으면 True",
        "async _fit_loop(symbols, interval_hours=6.0): background loop — 주기적으로 fit_all() 실행. EngineManager에서 시작",
        "data_provider=None일 때: fit 불가, 모든 forecast None, 모든 regime NORMAL. 하위 호환",
        "Add VolatilityService to src/bot/risk/__init__.py exports (파일 없으면 생성)",
        "Create tests/risk/test_volatility_service.py: mock data_provider로 fit_symbol (성공/실패), get_forecast, get_regime, get_market_regime, needs_refit, fit_all with partial failures. data_provider=None일 때 graceful degradation",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "GARCHModel.fit()은 arch 패키지 사용 (이미 설치됨). fit이 실패할 수 있음 (데이터 부족, 수렴 실패) — 반드시 try/except. VolatilityService는 singleton처럼 사용될 것이지만 DI로 주입. _fit_loop의 첫 실행은 시작 후 1분 후."
    },
    {
      "id": "V6-006",
      "title": "VaR/CVaR 포트폴리오 리스크 한도 — 실시간 리스크 게이트",
      "description": "기존 PortfolioRiskManager에 VaR가 있지만 실질적으로 활용되지 않는다. 실제 포지션 수익률 데이터와 quant/risk_metrics.py의 VaR/CVaR 함수를 연결하여, 포트폴리오 레벨에서 실시간 리스크 한도를 강제하고 pre-trade 리스크 체크를 수행한다.",
      "acceptanceCriteria": [
        "PortfolioRiskManager 확장: __init__에 volatility_service: VolatilityService | None = None 파라미터 추가 (기본 None, 하위 호환)",
        "Method calculate_parametric_var() -> float | None: quant.risk_metrics.parametric_var()를 포트폴리오 수익률에 적용. 가중평균 포트폴리오 수익률 사용",
        "Method calculate_cornish_fisher_var() -> float | None: quant.risk_metrics.cornish_fisher_var()를 포트폴리오 수익률에 적용. 비정규 분포 보정",
        "Method calculate_cvar() -> float | None: quant.risk_metrics.cvar() 적용. 최악 시나리오 기대 손실",
        "Method calculate_stress_var(n_simulations=1000) -> float | None: Monte Carlo 시뮬레이션 기반 VaR. 상관행렬 사용하여 correlated returns 생성. numpy로 구현 (외부 라이브러리 불필요)",
        "Method pre_trade_var_check(symbol: str, position_value: float) -> tuple[bool, str]: 현재 포트폴리오에 새 포지션 추가 시 VaR가 한도 초과하는지 시뮬레이션. 초과하면 (False, reason) 반환",
        "기존 validate_new_position()에 pre_trade_var_check() 추가 (var_enabled=True일 때만). 기존 check_var_limit() 대체 또는 보강",
        "get_risk_metrics()에 parametric_var, cornish_fisher_var, cvar, stress_var 필드 추가",
        "Config: var_method: str = 'historical' (historical|parametric|cornish_fisher), stress_var_simulations: int = 1000",
        "SETTINGS_METADATA에 위 2개 설정 추가 (section: 'Risk Management')",
        "GET /api/risk/portfolio 엔드포인트 추가: 모든 리스크 메트릭 반환 (exposure, heat, VaR 3종, CVaR, positions)",
        "Create tests/risk/test_var_integration.py: calculate_parametric_var, cornish_fisher, cvar, stress_var (seeded random), pre_trade_var_check (pass/fail), get_risk_metrics 포맷 검증",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "stress_var는 Cholesky decomposition으로 correlated returns 생성: L = cholesky(corr_matrix), simulated = L @ random_normal. 시뮬레이션 수가 많으면 느려지므로 default 1000. calculate_portfolio_var() 기존 메서드는 유지 (historical method)."
    },
    {
      "id": "V6-007",
      "title": "변동성 기반 동적 포지션 사이징 — GARCH + ATR 통합",
      "description": "현재 포지션 사이징은 고정 비율(risk_per_trade_pct) 또는 단순 ATR 기반이다. VolatilityService의 GARCH 예측을 활용하여 변동성이 높을 때 포지션을 줄이고, 낮을 때 늘리는 동적 사이징을 구현한다.",
      "acceptanceCriteria": [
        "Create src/bot/risk/dynamic_sizer.py with DynamicPositionSizer class",
        "DynamicPositionSizer.__init__(self, volatility_service: VolatilityService | None = None, portfolio_risk: PortfolioRiskManager | None = None, base_risk_pct: float = 1.0, vol_scale_factor: float = 1.0)",
        "Method calculate_size(symbol: str, price: float, portfolio_value: float, atr: float | None = None) -> PositionSize: PositionSize dataclass with quantity, notional_value, risk_amount, vol_multiplier, method ('garch'|'atr'|'fixed')",
        "사이징 로직: (1) GARCH forecast 있으면: vol_multiplier = median_vol / current_forecast_vol (변동성 높으면 < 1.0, 낮으면 > 1.0). (2) GARCH 없으면 ATR 기반: vol_multiplier = target_atr / actual_atr. (3) 둘 다 없으면: vol_multiplier = 1.0 (fixed). risk_amount = portfolio_value * base_risk_pct/100 * vol_multiplier. quantity = risk_amount / (price * vol_scale_factor)",
        "vol_multiplier 범위 제한: [0.25, 2.0] — 최소 25%, 최대 200% 스케일링",
        "Method validate_size(position_size: PositionSize, max_pct: float) -> PositionSize: portfolio_value 대비 max_pct 초과하면 클리핑",
        "BaseEngine에 _dynamic_sizer: DynamicPositionSizer | None = None 필드 + set_sizer(sizer) 메서드 추가",
        "4개 엔진(_check_entry 또는 포지션 사이징 부분)에서 _dynamic_sizer가 있으면 calculate_size() 사용. 없으면 기존 고정 사이징 유지",
        "DecisionStep에 사이징 결과 로깅: label='포지션 사이징', observation에 method/vol_multiplier/quantity 포함",
        "main.py에서 DynamicPositionSizer 생성하여 각 엔진에 set_sizer() 호출",
        "Config: dynamic_sizing_enabled: bool = True, vol_scale_factor: float = 1.0, max_position_scale: float = 2.0",
        "SETTINGS_METADATA에 위 3개 설정 추가 (section: 'Risk Management')",
        "Create tests/risk/test_dynamic_sizer.py: GARCH 모드 (high vol → 작은 포지션, low vol → 큰 포지션), ATR fallback, fixed fallback, vol_multiplier bounds [0.25, 2.0], validate_size 클리핑, DecisionStep 생성",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "엔진들의 기존 사이징 로직은 건드리지 않음. _dynamic_sizer=None이면 기존과 100% 동일. GARCH forecast가 0이면 vol_multiplier=1.0으로 fallback. median_vol은 conditional_volatility의 중간값."
    },
    {
      "id": "V6-008",
      "title": "엔진간 상관관계 리스크 컨트롤러",
      "description": "4개 엔진이 독립적으로 포지션을 잡지만, 같은 심볼(예: BTC/USDT)에 대해 여러 엔진이 동시에 포지션을 가지면 실질적 리스크가 집중된다. 엔진간 포지션 상관관계를 실시간 모니터링하고, 과도한 집중 시 경고/제한하는 컨트롤러를 만든다.",
      "acceptanceCriteria": [
        "Create src/bot/risk/correlation_controller.py with CorrelationRiskController class",
        "CorrelationRiskController.__init__(self, portfolio_risk: PortfolioRiskManager, max_cross_engine_correlation: float = 0.85, max_symbol_concentration: float = 0.4)",
        "Method update_positions(engine_positions: dict[str, list[dict]]): 엔진별 현재 포지션 맵 업데이트. 각 dict는 {symbol, side, notional}",
        "Method calculate_cross_engine_correlation() -> dict: 엔진 페어별 포지션 중복도 계산. {'funding_arb|grid_trading': {overlap_symbols: [...], overlap_pct: float, concentration_score: float}}",
        "Method check_symbol_concentration(symbol: str) -> tuple[bool, str]: 특정 심볼에 대한 전체 엔진 노출이 max_symbol_concentration(총 자본 대비) 초과하면 (False, reason)",
        "Method get_concentration_report() -> dict: {per_symbol: {symbol: {engines: [...], total_notional, pct_of_capital}}, cross_engine_correlations: {...}, alerts: [...]}",
        "EngineManager에서 매 사이클 콜백 후 correlation_controller.update_positions() 호출",
        "BaseEngine._has_capacity()에 cross-engine 체크 추가: correlation_controller가 있고 check_symbol_concentration() 실패하면 False 반환. controller 없으면 기존 동작",
        "EngineManager._rebalance_loop()에서 concentration_report를 참고하여, 집중도 높은 엔진의 allocation을 줄임",
        "GET /api/risk/correlation 엔드포인트 추가: concentration_report 반환",
        "Config: cross_engine_correlation_enabled: bool = True, max_symbol_concentration_pct: float = 40.0",
        "SETTINGS_METADATA에 위 2개 설정 추가 (section: 'Risk Management')",
        "Create tests/risk/test_correlation_controller.py: update_positions, cross_engine_correlation 계산, symbol_concentration (pass/fail), concentration_report 포맷, _has_capacity 통합",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "cross_engine_correlation은 포지션 심볼 기반 계산 (가격 상관관계 아님). 같은 BTC/USDT를 3개 엔진이 가지면 concentration=high. check_symbol_concentration은 비교적 lightweight하므로 매 사이클 호출 가능."
    },
    {
      "id": "V6-009",
      "title": "Trade Explorer 페이지 — 개별 거래 상세 분석",
      "description": "현재 Trades 페이지는 거래 리스트만 표시한다. 개별 거래를 클릭하면 진입/청산 가격, PnL 분해(gross/cost/net), 보유 시간, 엔진 의사결정 과정(DecisionStep), 동일 심볼 히스토리 등을 상세히 볼 수 있는 Trade Explorer를 만든다.",
      "acceptanceCriteria": [
        "Create frontend/src/pages/TradeExplorer.tsx",
        "트레이드 테이블: 필터링 (engine 선택, symbol 검색, 기간 선택, win/loss 토글), 정렬 (PnL, 날짜, 보유시간), 페이지네이션 (20건/페이지)",
        "트레이드 상세 패널 (행 클릭 시 확장 또는 사이드 패널): entry_price, exit_price, quantity, gross_pnl, cost, net_pnl을 분해 표시. 보유시간 (formatted: Xh Ym). engine_name, symbol, side 표시",
        "PnL 분해 바 차트: gross → cost → net을 waterfall 형태로 시각화 (recharts Bar). 양수는 초록, 음수는 빨강",
        "같은 심볼의 과거 거래 히스토리 (최근 10건) 미니 테이블",
        "CSV 내보내기 버튼: 현재 필터링된 거래를 CSV로 다운로드. 컬럼: timestamp, engine, symbol, side, entry_price, exit_price, quantity, gross_pnl, cost, net_pnl, hold_time",
        "Backend: GET /api/trades/detail 엔드포인트 추가 (또는 기존 /api/trades 확장): engine, symbol, start, end, win_only, limit, offset 파라미터 지원. EngineTracker._trades에서 조회",
        "GET /api/trades/export?format=csv — CSV 형태로 거래 내보내기",
        "Add /trade-explorer route in App.tsx with nav link (Trades 서브메뉴 또는 별도)",
        "다크 테마, 반응형 레이아웃 (모바일에서 상세 패널은 전체 화면)",
        "Frontend builds: cd frontend && npm run build",
        "Create tests for new API endpoints (test_trade_detail_api.py): 필터링, 페이지네이션, CSV export 포맷",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "EngineTracker._trades 데이터를 API로 노출. CSV export는 서버에서 text/csv 응답 생성. 프론트엔드의 CSV 다운로드는 Blob + URL.createObjectURL 패턴 사용. 기존 Trades 페이지는 유지하고, TradeExplorer는 추가 페이지."
    },
    {
      "id": "V6-010",
      "title": "Performance Heatmap — 시간대별/요일별 수익 분석",
      "description": "시간대(0-23시)별, 요일(월-일)별 PnL 분포를 히트맵으로 시각화한다. 트레이더가 '어떤 시간에 수익이 나고 어떤 시간에 손실이 나는지' 한눈에 파악하게 해준다. 추가로 심볼별 × 엔진별 수익 매트릭스와 월별 수익 캘린더도 제공한다.",
      "acceptanceCriteria": [
        "Create frontend/src/pages/Heatmaps.tsx",
        "시간대 × 요일 PnL 히트맵: 7행(Mon-Sun) × 24열(0-23h) 그리드. 각 셀 = 해당 시간대+요일의 총 PnL. 색상: 진한 초록(+) ~ 흰색(0) ~ 진한 빨강(-). recharts가 heatmap 미지원이면 커스텀 SVG/div 그리드로 구현",
        "엔진 × 심볼 수익 매트릭스: 행=엔진, 열=심볼. 셀=총 net_pnl. 동일한 색상 스케일",
        "월별 수익 캘린더: 12개월 그리드, 각 월의 총 PnL 표시. 현재 연도 기본, 연도 선택 가능",
        "Backend: GET /api/analytics/heatmap?type=hourly_dow — {data: [{hour, dow, pnl, trade_count}]}. EngineTracker._trades에서 exit_time 기준으로 hour/dow 계산",
        "GET /api/analytics/heatmap?type=engine_symbol — {data: [{engine, symbol, pnl, trade_count}]}",
        "GET /api/analytics/heatmap?type=monthly — {data: [{year, month, pnl, trade_count}]}",
        "각 히트맵 위에 토글: 전체/엔진별 필터, 기간 필터 (7d/30d/90d/all)",
        "셀 hover 시 툴팁: PnL 값, 거래 수, 승률",
        "Add /heatmaps route in App.tsx with nav link (Analytics 섹션)",
        "다크 테마, 반응형",
        "Frontend builds: cd frontend && npm run build",
        "Create tests/dashboard/test_heatmap_api.py: 3가지 heatmap 타입 엔드포인트 응답 포맷 검증",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "시간대×요일 히트맵은 7×24=168개 셀. 빈 셀은 PnL=0, count=0. recharts에 HeatMap이 없으므로 div 기반 커스텀 그리드 추천. 각 셀 크기: min-width 30px, height 30px. 색상 스케일은 min/max PnL 기준 동적 계산."
    },
    {
      "id": "V6-011",
      "title": "Risk Dashboard 페이지 — VaR/드로다운/상관관계 시각화",
      "description": "포트폴리오 리스크 현황을 한눈에 볼 수 있는 전용 대시보드를 만든다. VaR 게이지, 드로다운 커브, 엔진간 상관관계 히트맵, 포지션 익스포저 분해를 시각화한다.",
      "acceptanceCriteria": [
        "Create frontend/src/pages/RiskDashboard.tsx",
        "VaR 게이지 카드: 현재 포트폴리오 VaR (3종: historical, parametric, cornish-fisher) + CVaR을 게이지/미터로 표시. 한도(max_portfolio_var_pct) 대비 현재 수준을 원형 게이지로 표현. 초록(안전)/노랑(주의)/빨강(위험) 색상",
        "드로다운 커브 차트: 시간에 따른 포트폴리오 드로다운 변화. Area chart (recharts). 현재 드로다운, 최대 드로다운, 회복까지 시간 표시",
        "엔진간 상관관계 히트맵: 4×4 매트릭스 (funding_arb, grid, cross_arb, stat_arb). 셀=포지션 심볼 기반 overlap 비율. /api/risk/correlation 데이터 사용",
        "포지션 익스포저 분해: (1) 엔진별 stacked bar (각 엔진의 총 notional), (2) 심볼별 pie chart (전체 포트폴리오에서 심볼별 비중), (3) 집중도 경고 (max_symbol_concentration 초과 시 빨간 배지)",
        "포트폴리오 히트: 현재 heat 값과 한도를 수평 bar로 표시",
        "Backend 데이터 소스: GET /api/risk/portfolio (V6-006에서 추가), GET /api/risk/correlation (V6-008에서 추가). 추가로 GET /api/risk/drawdown → PortfolioManager에서 drawdown history time-series",
        "PortfolioManager에 _drawdown_history: list[dict] 추가: 매 PnL 보고 시 {timestamp, drawdown_pct, equity} 기록 (최근 1000건)",
        "GET /api/risk/drawdown 엔드포인트 추가",
        "Add /risk route in App.tsx with nav link",
        "15초 자동 새로고침",
        "다크 테마, 반응형",
        "Frontend builds: cd frontend && npm run build",
        "Create tests for drawdown history endpoint",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": "VaR 게이지는 recharts의 RadialBarChart 또는 커스텀 SVG arc. 드로다운은 음수 영역 Area chart (fill red). 리스크 API가 아직 V6-006, V6-008에서 안 만들어졌을 수 있으므로, 해당 엔드포인트가 404이면 graceful degradation (데이터 없음 표시)."
    },
    {
      "id": "V6-012",
      "title": "메트릭 영속화 레이어 — EngineTracker → SQLite 저장",
      "description": "현재 EngineTracker는 메모리에만 데이터를 보관하여 재시작 시 모두 사라진다. SQLAlchemy 모델을 추가하여 트레이드 기록과 메트릭 스냅샷을 DB에 저장하고, 시작 시 복원한다.",
      "acceptanceCriteria": [
        "Add to src/bot/data/models.py: EngineTradeRecord 모델 — engine_name, symbol, side, entry_price, exit_price, quantity, pnl (gross), cost, net_pnl, entry_time, exit_time, hold_time_seconds. Index on (engine_name, exit_time)",
        "Add to src/bot/data/models.py: EngineMetricSnapshot 모델 — engine_name, timestamp, total_trades, winning_trades, losing_trades, win_rate, total_pnl, sharpe_ratio, max_drawdown, profit_factor, cost_ratio. Index on (engine_name, timestamp)",
        "Create src/bot/engines/metrics_persistence.py with MetricsPersistence class",
        "MetricsPersistence.__init__(self, data_store: DataStore, tracker: EngineTracker)",
        "Method async save_trade(engine_name: str, trade: TradeRecord): EngineTracker의 TradeRecord를 EngineTradeRecord로 변환하여 DB에 저장",
        "Method async save_metrics_snapshot(): 모든 엔진의 현재 EngineMetrics를 EngineMetricSnapshot으로 저장",
        "Method async load_trades(since_hours=24) -> dict[str, list[TradeRecord]]: DB에서 최근 트레이드를 로드하여 EngineTracker 형식으로 변환",
        "Method async restore_tracker(): 시작 시 호출 — DB에서 최근 24시간 트레이드를 로드하여 tracker에 복원. tracker에 bulk_load_trades(trades) 메서드 추가 필요",
        "EngineTracker에 bulk_load_trades(engine_name: str, trades: list[TradeRecord]) 메서드 추가: _trades와 _pnl_history를 복원",
        "Method async _snapshot_loop(interval_minutes=5): background loop — 주기적으로 save_metrics_snapshot() 실행",
        "Method async cleanup(max_days=90): 90일 이상 된 EngineTradeRecord, EngineMetricSnapshot 삭제",
        "EngineManager에서 MetricsPersistence 생성, tracker와 연결. record_trade 콜백에서 persistence.save_trade() 호출. start_background_loops()에서 _snapshot_loop() 시작",
        "시작 시 restore_tracker() 호출하여 이전 세션 데이터 복원",
        "Config: metrics_persistence_enabled: bool = True, metrics_snapshot_interval_minutes: float = 5.0, metrics_retention_days: int = 90",
        "SETTINGS_METADATA에 위 3개 설정 추가 (section: 'Metrics')",
        "Create tests/engines/test_metrics_persistence.py: save_trade, save_metrics_snapshot, load_trades, restore_tracker, cleanup, bulk_load_trades",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": "DB 마이그레이션: Base.metadata.create_all()이 새 테이블 자동 생성. 기존 테이블은 건드리지 않음. restore_tracker()는 첫 시작 시만 실행 (이후는 실시간 저장). EngineTracker의 TradeRecord와 data.models의 TradeRecord(거래소 주문)는 다른 모델이므로 이름 충돌 주의 — DB 모델을 EngineTradeRecord로 구분."
    },
    {
      "id": "V6-013",
      "title": "히스토리컬 분석 API + 프론트엔드 — 장기 성과 조회",
      "description": "영속화된 메트릭을 활용하여 장기(일/주/월) 성과 분석 API를 제공하고, Performance 페이지에 날짜 범위 선택과 히스토리컬 차트를 추가한다.",
      "acceptanceCriteria": [
        "DataStore에 추가 쿼리 메서드: get_engine_metric_snapshots(engine_name, start, end, limit=500) -> list[dict], get_engine_trades(engine_name, start, end, limit=500) -> list[dict]",
        "GET /api/metrics/history?engine={name}&days={n} 엔드포인트: EngineMetricSnapshot 시계열 반환. 응답: {timestamps: [], sharpe: [], win_rate: [], total_pnl: [], max_drawdown: []}",
        "GET /api/metrics/compare?engines=a,b,c&metric=sharpe&days=30 엔드포인트: 여러 엔진의 특정 메트릭 시계열 비교. 응답: {engines: {name: {timestamps: [], values: []}}}",
        "GET /api/metrics/daily-summary?days=30 엔드포인트: 일별 종합 요약. 응답: {daily: [{date, total_pnl, total_trades, avg_sharpe, total_cost}]}",
        "frontend/src/pages/Performance.tsx 업데이트: (1) 날짜 범위 선택 UI (7d/30d/90d/custom), (2) 히스토리컬 PnL 차트 — 일별 막대 차트 (양수 초록/음수 빨강), (3) Sharpe/Win Rate 트렌드 라인 차트 (다중 엔진 겹쳐 표시), (4) 엔진 비교 차트 — 선택한 메트릭으로 엔진들 비교",
        "차트에 주석 표시: 파라미터 변경 시점 (tuner/deployer history), 레짐 전환 시점 — 수직 점선으로",
        "Loading skeleton, 빈 데이터 상태 ('아직 데이터가 충분하지 않습니다')",
        "Frontend builds: cd frontend && npm run build",
        "Create tests/dashboard/test_metrics_history_api.py: history, compare, daily-summary 엔드포인트 응답 포맷 검증",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": "EngineMetricSnapshot이 5분마다 저장되므로 30일 = 8640건. daily-summary는 GROUP BY date로 집계. compare에서 시간축 정렬 (start/end 맞추기). 주석 데이터는 tuner.get_history()와 deployer.get_deploy_history()에서 가져옴."
    },
    {
      "id": "V6-014",
      "title": "시장 레짐 탐지 서비스 — 실시간 레짐 분류 + 히스토리",
      "description": "VolatilityService의 레짐 분류를 확장하여 전용 MarketRegimeDetector를 만든다. BTC를 시장 대리변수로 사용하여 전체 시장 레짐을 실시간 판별하고, 레짐 전환 히스토리를 기록한다. CRISIS 레짐 추가로 극단적 시장 상황 감지.",
      "acceptanceCriteria": [
        "Create src/bot/risk/regime_detector.py with MarketRegimeDetector class",
        "MarketRegime enum: LOW, NORMAL, HIGH, CRISIS (기존 VolatilityRegime의 3단계에 CRISIS 추가)",
        "MarketRegimeDetector.__init__(self, volatility_service: VolatilityService, crisis_threshold: float = 2.5, lookback_window: int = 30)",
        "Method detect_regime() -> MarketRegime: (1) volatility_service.get_market_regime() 기본값 사용, (2) 추가로 CRISIS 판별: 최근 변동성이 장기 평균의 crisis_threshold배 초과하면 CRISIS. BTC 24h 변화율이 -10% 이하여도 CRISIS",
        "Method get_current_regime() -> MarketRegime: 캐시된 현재 레짐 반환",
        "Method get_regime_history() -> list[dict]: [{timestamp, regime, duration_minutes, trigger}] 형태의 레짐 전환 히스토리. 최근 100건 보관",
        "Method get_regime_duration() -> float: 현재 레짐이 지속된 시간(분)",
        "Method is_crisis() -> bool: 현재 레짐이 CRISIS인지 빠르게 체크",
        "async _detection_loop(interval_seconds=300): 5분마다 detect_regime() 실행. 레짐 전환 시 로깅 + 히스토리 기록",
        "EngineManager에서 MarketRegimeDetector 생성, _detection_loop() background task로 시작",
        "GET /api/market/regime 엔드포인트: {current: 'NORMAL', since: 'ISO timestamp', duration_minutes: 45, history: [...last 20 transitions]}",
        "WebSocket broadcast에 regime 정보 포함 (status_update에 'market_regime' 필드 추가)",
        "Config: regime_detection_enabled: bool = True, regime_crisis_threshold: float = 2.5, regime_detection_interval_seconds: float = 300.0",
        "SETTINGS_METADATA에 위 3개 설정 추가 (section: 'Market Regime')",
        "Create tests/risk/test_regime_detector.py: detect_regime (각 레짐 조건), CRISIS 조건, regime_history 기록, is_crisis(), get_regime_duration(), detection loop",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": "CRISIS 판별은 보수적으로: 변동성 2.5배 이상 또는 24h -10% 이하. 양쪽 다 만족할 필요 없음 (OR 조건). volatility_service가 None이거나 fit 안 됐으면 항상 NORMAL. detect_regime() 실패 시 이전 레짐 유지."
    },
    {
      "id": "V6-015",
      "title": "BaseEngine 레짐 조절 인터페이스 + 4개 엔진 적용",
      "description": "MarketRegimeDetector의 레짐 정보를 각 엔진이 읽어서 threshold/size를 자동 조절한다. BaseEngine에 공통 인터페이스를 추가하고, 4개 엔진에 적용한다.",
      "acceptanceCriteria": [
        "BaseEngine에 _regime_detector: MarketRegimeDetector | None = None 필드 + set_regime_detector(detector) 메서드 추가",
        "BaseEngine에 _get_regime_adjustments() -> dict 메서드: LOW={threshold_mult:0.8, size_mult:1.2}, NORMAL={1.0, 1.0}, HIGH={1.3, 0.7}, CRISIS={999, 0.0}. detector 없으면 NORMAL 반환",
        "funding_arb._run_cycle(): min_rate에 threshold_mult 곱함. CRISIS면 신규 진입 skip",
        "grid_trading._run_cycle(): grid_spacing에 threshold_mult 적용. CRISIS면 새 그리드 주문 안 넣음",
        "cross_exchange_arb._run_cycle(): min_spread_pct에 threshold_mult 적용. CRISIS면 신규 진입 skip",
        "stat_arb._run_cycle(): entry_zscore에 threshold_mult 적용. CRISIS면 신규 진입 skip",
        "각 엔진 cycle 시작에 DecisionStep: label='시장 레짐', observation='현재: {regime}', category='evaluate'",
        "Create tests/engines/test_regime_adaptation.py: 각 레짐별 threshold/size 조절 검증, detector=None이면 무조절",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 15,
      "passes": true,
      "notes": "detector=None이면 기존과 100% 동일 동작. threshold_mult=999는 사실상 진입 불가. 기존 엔진 테스트 깨지지 않게 주의."
    },
    {
      "id": "V6-016",
      "title": "서킷 브레이커 + main.py 와이어링",
      "description": "CRISIS 레짐이 일정 시간 이상 지속되면 모든 엔진을 자동 pause하는 서킷 브레이커를 EngineManager에 추가하고, main.py에서 regime_detector를 각 엔진에 연결한다.",
      "acceptanceCriteria": [
        "EngineManager에 _circuit_breaker_check() 메서드: regime_detector.is_crisis()가 True이고 duration >= crisis_circuit_breaker_minutes이면 모든 엔진 pause(). CRISIS 해제되면 resume()",
        "EngineManager._detection_loop 또는 별도 _circuit_breaker_loop()에서 주기적(1분) 체크",
        "서킷 브레이커 발동/해제 시 audit log 기록",
        "main.py에서 regime_detector를 각 엔진에 set_regime_detector() 호출",
        "Config: regime_adaptation_enabled: bool = True, crisis_circuit_breaker_minutes: float = 30.0",
        "SETTINGS_METADATA에 위 2개 설정 추가 (section: 'Market Regime')",
        "Create tests/engines/test_circuit_breaker.py: CRISIS 30분 이상 → pause, CRISIS 해제 → resume, adaptation_enabled=False면 미동작",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 16,
      "passes": true,
      "notes": "서킷 브레이커는 EngineManager의 background task. pause()는 BaseEngine에 이미 있음."
    },
    {
      "id": "V6-017",
      "title": "Dockerfile + docker-compose + .dockerignore",
      "description": "프로덕션 배포를 위한 Docker 컨테이너화. 멀티 스테이지 빌드, non-root user, health check.",
      "acceptanceCriteria": [
        "Create Dockerfile: 멀티 스테이지. Stage 1 (frontend-builder): node:20-alpine, npm ci && npm run build. Stage 2: python:3.11-slim, pip install, COPY src/ + frontend dist. non-root user. EXPOSE 8000",
        "Create docker-compose.yml: service 'bot', ports 8000:8000, env_file .env, volumes ./data:/app/data, restart unless-stopped, healthcheck curl /api/health, stop_grace_period 35s",
        "Create .dockerignore: node_modules, __pycache__, .env, .git, data/*.db, venv",
        "requirements.txt를 pyproject.toml과 동기화 확인",
        "환경변수 TRADING_ENV (production|staging|development) 기반 로깅 레벨 분리",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 17,
      "passes": true,
      "notes": "Docker 없어도 파일이 정확하면 충분. docker build는 실행하지 않아도 됨."
    },
    {
      "id": "V6-018",
      "title": "Health 엔드포인트 + graceful shutdown",
      "description": "상세 health check 엔드포인트와 SIGTERM/SIGINT graceful shutdown 처리.",
      "acceptanceCriteria": [
        "GET /api/health 엔드포인트 강화 (인증 불필요): status (healthy/degraded/unhealthy), uptime_seconds, engines 상태, database 연결. detailed=true면 disk_space_mb, memory_usage_mb 추가",
        "Health 로직: 모든 엔진 RUNNING → healthy, 일부 ERROR → degraded, 전부 STOPPED → unhealthy",
        "main.py graceful shutdown: SIGTERM/SIGINT → 엔진 stop → 메트릭 스냅샷 → DB 정리. timeout 30초",
        "Config: shutdown_timeout_seconds: float = 30.0",
        "Create tests/dashboard/test_health_endpoint.py: healthy/degraded/unhealthy 조건, detailed 파라미터",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 18,
      "passes": false,
      "notes": "health 엔드포인트는 V6-014에서 기본 버전이 이미 있을 수 있음. 강화만 하면 됨."
    },
    {
      "id": "V6-019",
      "title": "최종 통합 검증 + V6 완료",
      "description": "V6 전체 모듈 통합 검증. 모든 테스트, 린트, 프론트엔드 빌드 통과를 확인한다.",
      "acceptanceCriteria": [
        "tests/test_v6_integration.py 생성: V6 전체 모듈 import 검증, 새 config 필드 기본값 검증, 주요 클래스 인스턴스화",
        "pytest tests/ -v — ALL 테스트 통과",
        "ruff check src/ tests/ — 린트 에러 0개",
        "cd frontend && npm run build — 프론트엔드 빌드 성공",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 19,
      "passes": false,
      "notes": "이 스토리가 V6의 마지막. 전체 통합 검증만 수행."
    }
  ]
}