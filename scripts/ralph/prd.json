{
  "project": "CoinTradingBot-V5-ProfitableTrading",
  "branchName": "ralph/v5-profitable-trading",
  "description": "Upgrade the trading bot to pursue realistic profitability: cost-aware engines with fee/slippage modeling, multi-pair trading expansion, performance tracking with auto-tuning, capital rebalancing, and an automated financial engineering research loop. All changes maintain backward compatibility with 1865+ existing tests.",
  "userStories": [
    {
      "id": "V5-001",
      "title": "CostModel — Trading cost calculator module",
      "description": "Create a reusable cost model that accurately calculates trading fees, slippage, and net profitability for any trade. This is the foundation for making all engines cost-aware.",
      "acceptanceCriteria": [
        "Create src/bot/engines/cost_model.py with CostModel dataclass",
        "CostModel fields: maker_fee_pct (default 0.02), taker_fee_pct (default 0.04), slippage_pct (default 0.01) — all as percentages (0.02 = 0.02%)",
        "Method round_trip_cost(notional, legs=2) -> float: returns total cost in USD = notional * (fee_pct + slippage_pct) * legs / 100. Use taker_fee_pct by default, accept is_maker param to use maker_fee_pct",
        "Method net_profit(gross_pnl, notional, legs=2) -> float: returns gross_pnl - round_trip_cost(notional, legs)",
        "Method min_spread_for_profit(legs=2) -> float: returns minimum spread percentage needed = (fee_pct + slippage_pct) * legs. This is the break-even spread",
        "Method is_profitable(gross_pnl, notional, legs=2) -> bool: returns True if net_profit > 0",
        "Add CostModel to src/bot/engines/__init__.py exports",
        "Create tests/engines/test_cost_model.py with comprehensive tests: round_trip_cost with 2 and 4 legs, net_profit positive/negative/break-even, min_spread_for_profit, is_profitable, custom fee values, maker vs taker fees",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Keep it simple. Default fees match Binance VIP0 spot. All percentages stored as raw values (0.02 means 0.02%), NOT as fractions (not 0.0002)."
    },
    {
      "id": "V5-002",
      "title": "Fee-aware engine integration — all 4 engines use CostModel",
      "description": "Integrate CostModel into all 4 trading engines so they calculate realistic PnL after fees and use dynamic thresholds instead of magic numbers.",
      "acceptanceCriteria": [
        "Each engine creates a CostModel instance in __init__ (use default fees or accept from config)",
        "funding_arb.py: Use CostModel with legs=4 (spot buy + perp sell + close spot + close perp). Replace fixed min_rate threshold with dynamic: only enter if expected funding income > cost_model.round_trip_cost(). Add DecisionStep showing cost calculation (label: '비용 분석', category: 'evaluate'). Deduct costs from PnL when closing positions",
        "grid_trading.py: Deduct maker fee from each grid fill profit. PnL per fill = (grid_spacing_value - cost_model.round_trip_cost(fill_notional, legs=2)) * size. Skip fills where net profit would be negative. Add DecisionStep for cost check on each fill",
        "cross_exchange_arb.py: Replace hardcoded min_spread_pct check with cost_model.min_spread_for_profit(legs=4) as minimum. Only execute when actual spread > min_spread_for_profit. Log the cost-adjusted minimum in DecisionStep",
        "stat_arb.py: Deduct round_trip_cost from PnL when closing positions. Replace magic number 0.005 with cost-based calculation. Add DecisionStep showing net vs gross PnL",
        "All cost-related decisions visible in cycle logs via DecisionStep (Korean labels for dashboard)",
        "Add new tests in tests/engines/ for each engine verifying: cost deduction from PnL, trade rejection when unprofitable after fees, DecisionStep includes cost info",
        "All existing engine tests still pass (CostModel uses defaults that don't change existing behavior — new params have defaults)",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "CRITICAL: Existing engine tests must not break. Add CostModel with default values. The key behavioral change is PnL now reflects fees. Existing tests that check PnL values may need updating — update the expected values to reflect fee deduction, don't remove the tests."
    },
    {
      "id": "V5-003",
      "title": "Multi-pair config expansion + engine description metadata",
      "description": "Expand default trading symbols for all engines and add engine description metadata to the backend for dashboard display.",
      "acceptanceCriteria": [
        "Update config.py default symbol lists: funding_arb_symbols=['BTC/USDT','ETH/USDT','SOL/USDT','XRP/USDT','DOGE/USDT'], grid_symbols=['BTC/USDT','ETH/USDT','SOL/USDT'], cross_arb_symbols=['BTC/USDT','ETH/USDT','SOL/USDT'], stat_arb_pairs=[['BTC/USDT','ETH/USDT'],['SOL/USDT','ETH/USDT']]",
        "Add ENGINE_DESCRIPTIONS dict in src/bot/engines/base.py or a new constants module — maps engine name to {role_ko, role_en, description_ko, key_params}: funding_rate_arb role_ko='델타중립 펀딩비 차익거래', grid_trading role_ko='그리드 자동매매', cross_exchange_arb role_ko='거래소간 차익거래', stat_arb role_ko='통계적 차익거래'. Include short descriptions explaining how each engine works",
        "Add GET /api/engines/{name}/params endpoint in dashboard/app.py: returns engine's current config params + description metadata + symbols being tracked",
        "Update GET /api/engines to include description and symbols in each engine's info dict",
        "Update SETTINGS_METADATA for any new/changed config fields",
        "Update frontend/src/api/types.ts: add description, role_ko, symbols fields to EngineInfo type",
        "Tests: verify /api/engines returns descriptions, /api/engines/{name}/params returns config values",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "The engines already have multi-symbol loops — we're just expanding the default config. The descriptions are static metadata, not dynamic."
    },
    {
      "id": "V5-004",
      "title": "Engine role cards with descriptions in frontend",
      "description": "Enhance the Engines.tsx page to show role descriptions, tracked symbols, and key parameters for each engine.",
      "acceptanceCriteria": [
        "Update frontend/src/pages/Engines.tsx: each engine card shows a collapsible description section at the top",
        "Description section contains: role name in Korean (bold), one-line English explanation, how-it-works description",
        "Below description: 'Tracked Symbols' section showing chips/badges for each symbol the engine monitors",
        "Key Parameters section: show engine-specific params (e.g., min_rate for funding_arb, grid_spacing for grid_trading) with current values",
        "Collapsible: description section starts collapsed, expandable via chevron button (saves screen space)",
        "Cost info: if engine has CostModel, show estimated round-trip cost per trade",
        "Use existing dark theme styling consistent with other pages",
        "Responsive layout: description section stacks on mobile",
        "Frontend builds successfully: cd frontend && npm run build",
        "All existing Python tests pass, ruff check passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "This is a frontend-only change (plus reading data from the /api/engines endpoint updated in V5-003). No new Python tests needed."
    },
    {
      "id": "V5-005",
      "title": "EngineTracker — performance tracking and metrics",
      "description": "Create a performance tracking module that records every engine cycle and trade, computing rolling metrics like win rate, Sharpe ratio, max drawdown, and profit factor.",
      "acceptanceCriteria": [
        "Create src/bot/engines/tracker.py with EngineTracker class",
        "TradeRecord dataclass: engine_name, symbol, side (buy/sell), entry_price, exit_price, quantity, pnl, cost, net_pnl, entry_time, exit_time, hold_time_seconds",
        "EngineMetrics dataclass: total_trades, winning_trades, losing_trades, win_rate, total_pnl, avg_profit_per_trade, sharpe_ratio (annualized), max_drawdown, profit_factor (total_wins/total_losses), avg_hold_time_min, cost_ratio (total_cost/total_gross_pnl), best_trade, worst_trade, total_cost",
        "EngineTracker.record_trade(engine_name, trade: TradeRecord): stores trade in per-engine list",
        "EngineTracker.record_cycle(engine_name, result: EngineCycleResult): stores cycle result for analysis",
        "EngineTracker.get_metrics(engine_name, window_hours=24) -> EngineMetrics: computes metrics from trades within time window",
        "EngineTracker.get_all_metrics(window_hours=24) -> dict[str, EngineMetrics]: metrics for all engines",
        "EngineTracker.get_pnl_history(engine_name) -> list[dict]: time-series of cumulative PnL for charting",
        "Sharpe ratio calculation: annualized = mean(returns) / std(returns) * sqrt(365*24) assuming hourly returns. Return 0.0 if insufficient data (<2 trades)",
        "Max drawdown: track peak equity, compute max drop from peak as percentage",
        "Add EngineTracker to src/bot/engines/__init__.py exports",
        "Create tests/engines/test_tracker.py: test metrics calculation with known trades (verify win_rate, sharpe, drawdown, profit_factor), test empty state returns zero metrics, test window filtering, test pnl_history",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "EngineTracker is in-memory only (no database). Data resets on restart. Sufficient for paper mode monitoring."
    },
    {
      "id": "V5-006",
      "title": "Tracker integration + performance API endpoints",
      "description": "Integrate EngineTracker into the EngineManager and expose performance metrics via dashboard API endpoints.",
      "acceptanceCriteria": [
        "Add EngineTracker instance to EngineManager (created in __init__)",
        "EngineManager: after each engine cycle callback, call tracker.record_cycle(). When engine reports a trade (PnL update in cycle result), create TradeRecord and call tracker.record_trade()",
        "Add to dashboard/app.py: GET /api/engines/{name}/metrics -> returns EngineMetrics as JSON (24h default, accepts ?hours= param)",
        "Add to dashboard/app.py: GET /api/performance/summary -> returns {engines: {name: EngineMetrics}, totals: {total_pnl, total_trades, overall_sharpe, overall_win_rate, total_cost}}",
        "Wire EngineManager.tracker into dashboard state via set_engine_manager() or similar pattern",
        "Include performance summary in WebSocket status_update broadcasts (abbreviated: per-engine pnl and win_rate)",
        "Update frontend/src/api/types.ts: add EngineMetrics interface, PerformanceSummary interface",
        "Tests: API endpoints return correct data, tracker records from cycle callbacks, summary aggregation works",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "The tracker is instantiated in EngineManager, not passed in from outside. Keep coupling minimal."
    },
    {
      "id": "V5-007",
      "title": "Performance dashboard page",
      "description": "Build a React performance dashboard showing per-engine PnL curves, key metrics, cost analysis, and capital allocation.",
      "acceptanceCriteria": [
        "Create frontend/src/pages/Performance.tsx",
        "Per-engine PnL curves: line chart (recharts) showing cumulative PnL over time for each engine, different colors per engine",
        "Metrics cards: for each engine show win_rate, sharpe_ratio, max_drawdown, profit_factor, total_trades, total_cost",
        "Cost analysis section: bar chart showing gross PnL vs costs vs net PnL per engine",
        "Capital allocation: pie chart showing current capital allocation across engines (from /api/engines data)",
        "Overall summary at top: total PnL, overall win rate, total cost ratio",
        "Auto-refresh via WebSocket or 10s polling",
        "Loading skeleton states, error states, empty data states",
        "Add /performance route in App.tsx with nav link",
        "Dark theme consistent with existing pages",
        "Frontend builds successfully: cd frontend && npm run build",
        "All existing Python tests pass, ruff check passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Data comes from GET /api/performance/summary and GET /api/engines/{name}/metrics endpoints created in V5-006."
    },
    {
      "id": "V5-008",
      "title": "ParameterTuner — automatic strategy parameter adjustment",
      "description": "Create an auto-tuner that evaluates engine performance and adjusts parameters to improve profitability.",
      "acceptanceCriteria": [
        "Create src/bot/engines/tuner.py with ParameterTuner class",
        "ParamChange dataclass: engine_name, param_name, old_value, new_value, reason, timestamp",
        "TunerConfig per engine defining tunable params with min/max bounds: funding_arb (min_rate: 0.0001-0.001, max_spread_pct: 0.1-1.0), grid_trading (grid_spacing_pct: 0.1-2.0, grid_levels: 5-20), cross_exchange_arb (min_spread_pct: 0.05-1.0), stat_arb (entry_zscore: 1.0-3.0, exit_zscore: 0.1-1.0, lookback: 50-200, min_correlation: 0.5-0.95)",
        "ParameterTuner.evaluate_and_adjust(engine_name, metrics: EngineMetrics, current_params: dict) -> list[ParamChange]: returns recommended changes",
        "Adjustment logic: if sharpe < 0 -> conservative (raise thresholds, reduce sizes); if 0 <= sharpe < 0.5 -> minor adjustments; if sharpe >= 1.0 -> can be more aggressive (lower thresholds slightly, within bounds); all adjustments are small (max 10-20% change per cycle)",
        "ParameterTuner.get_history(engine_name) -> list[ParamChange]: returns adjustment history",
        "ParameterTuner.apply_changes(changes: list[ParamChange], settings: Settings): applies param changes via settings.reload()",
        "All adjustments logged as DecisionSteps for dashboard visibility",
        "Add ParameterTuner to src/bot/engines/__init__.py exports",
        "Tests: test sharpe < 0 raises thresholds, sharpe > 1 lowers them, bounds are respected, history tracking works, apply_changes calls settings.reload correctly",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Tuner does NOT modify engines directly — it changes Settings values via reload(), which engines read on next cycle. Max adjustment per cycle is bounded to prevent wild swings."
    },
    {
      "id": "V5-009",
      "title": "Capital rebalance + tuner/rebalance integration in manager",
      "description": "Add performance-based capital rebalancing to PortfolioManager and integrate the tuner and rebalance loops into EngineManager.",
      "acceptanceCriteria": [
        "Add rebalance_allocations(metrics: dict[str, EngineMetrics]) method to PortfolioManager: computes Sharpe-weighted allocation with min 10% max 40% per engine, updates engine_allocations",
        "Rebalance logic: weight_i = max(sharpe_i, 0.1) for each engine; normalize weights to sum to 1.0; clip each to [0.10, 0.40]; re-normalize after clipping",
        "Add _tuner_loop() to EngineManager: runs every 24 hours, calls tuner.evaluate_and_adjust() for each engine using tracker metrics, applies changes",
        "Add _rebalance_loop() to EngineManager: runs every 24 hours (offset from tuner), calls portfolio_manager.rebalance_allocations() with tracker metrics",
        "Both loops log their actions as DecisionSteps and emit WebSocket broadcasts",
        "Add GET /api/engines/{name}/params endpoint (or update existing): include tuner adjustment history in response",
        "Add rebalance history to GET /api/performance/summary response",
        "Config: add tuner_enabled (default True, hot-reloadable), tuner_interval_hours (default 24), rebalance_enabled (default True, hot-reloadable)",
        "Tests: rebalance computes correct weights, min/max bounds enforced, tuner loop triggers on schedule, integration with tracker metrics",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "The tuner and rebalance run as background tasks in EngineManager, similar to how engines run as asyncio tasks. First run after 1 hour, then every 24 hours."
    },
    {
      "id": "V5-010",
      "title": "Research framework — base module, experiments, and backtest runner",
      "description": "Create an automated financial engineering research framework with experiment base classes and 4 initial experiments.",
      "acceptanceCriteria": [
        "Create src/bot/research/ directory with __init__.py",
        "Create src/bot/research/base.py: ResearchTask ABC with run_experiment() -> ResearchReport and apply_findings(engine) -> list[ParamChange] abstract methods. target_engine property",
        "Create src/bot/research/report.py: ResearchReport dataclass with experiment_name, hypothesis, methodology, data_period, results (dict), conclusion, recommended_changes (list[ParamChange]), improvement_significant (bool), timestamp",
        "Create src/bot/research/backtest_runner.py: SimpleBacktestRunner class that runs a basic simulation over historical price data. Methods: run(prices, strategy_fn) -> BacktestResult with returns, sharpe, max_drawdown",
        "Create src/bot/research/experiments/__init__.py",
        "Create src/bot/research/experiments/volatility_regime.py: VolatilityRegimeExperiment — uses ATR to detect high/low volatility, tests whether dynamic grid spacing (wider in high vol) improves Sharpe. Target: grid_trading, stat_arb",
        "Create src/bot/research/experiments/cointegration.py: CointegrationExperiment — runs Engle-Granger cointegration test on price pairs to identify truly cointegrated pairs. Target: stat_arb",
        "Create src/bot/research/experiments/optimal_grid.py: OptimalGridExperiment — simulates different grid_spacing/levels combinations on historical data to find optimal params. Target: grid_trading",
        "Create src/bot/research/experiments/funding_prediction.py: FundingPredictionExperiment — analyzes historical funding rate patterns (time-of-day, day-of-week) to predict direction. Target: funding_arb",
        "Each experiment generates a ResearchReport with hypothesis, methodology, results dict, and conclusion string",
        "Tests in tests/research/: test base class interface, test report serialization, test each experiment runs without errors (with mock/synthetic data), test backtest runner with simple strategy",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 10,
      "passes": false,
      "notes": "Experiments use synthetic/mock data for now (no real exchange data needed). Cointegration uses statsmodels (already in dependencies). ATR calculation can use simple numpy. Keep experiments self-contained — they should work with just a price array input."
    },
    {
      "id": "V5-011",
      "title": "Research loop integration + research dashboard + final verification",
      "description": "Integrate the research framework into EngineManager, add a research dashboard page, API endpoints, and run final verification.",
      "acceptanceCriteria": [
        "Add _research_loop() to EngineManager: runs every 24 hours, iterates over registered experiments, runs each, stores reports, applies significant findings via tuner",
        "Store research reports in EngineManager._research_reports list (in-memory)",
        "Add GET /api/research/experiments -> list of experiment names + status (pending/running/completed/failed)",
        "Add GET /api/research/reports -> list of ResearchReport dicts, most recent first",
        "Config: add research_enabled (default True, hot-reloadable), research_interval_hours (default 24)",
        "Create frontend/src/pages/Research.tsx: shows experiment list with status, each report displays hypothesis, methodology, results, conclusion, recommended changes, timestamp",
        "Add /research route in App.tsx with nav link",
        "Research page: color-coded results (green if improvement_significant, gray otherwise), param changes shown as 'before -> after' format",
        "Dark theme consistent with existing pages",
        "Frontend builds successfully: cd frontend && npm run build",
        "Create tests/engines/test_research_integration.py or tests/research/test_integration.py: verify research loop runs experiments, stores reports, applies findings",
        "Final verification: pytest tests/ -v passes ALL tests (old + new), ruff check src/ tests/ clean",
        "All existing tests pass, ruff check passes"
      ],
      "priority": 11,
      "passes": false,
      "notes": "This is the final story. After this, all V5 features are complete. The research loop uses synthetic data for experiments — no real exchange API calls needed."
    }
  ]
}
