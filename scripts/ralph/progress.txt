# Ralph Progress Log
Started: 2026년 2월 23일 월요일 17시 18분 03초 KST
---

## Codebase Patterns

### Circular Import: research ↔ engines
- `bot.research.base` → `bot.engines.tuner` (ParamChange) → `bot.engines.__init__` → `bot.engines.manager` → `bot.research.base` (circular!)
- Fix: `bot.research/__init__.py` uses lazy `__getattr__` imports instead of module-level imports
- Existing tests work only because they import engines before research (by coincidence)
- Any new module in research/ can be imported directly (`from bot.research.data_provider import X`) without triggering the circular chain

### DataStore Patterns
- `get_candles()` returns `list[OHLCV]` (Pydantic models), sorted oldest→newest
- `get_funding_rates()` returns `list[dict]` with keys: symbol, timestamp, funding_rate, funding_timestamp, mark_price, spot_price, spread_pct
- New query methods: just add async methods using `self._session_factory()` context manager
- `func.count()` from sqlalchemy for aggregate queries

### Testing Patterns
- Use `MagicMock()` for DataStore, set async methods via `AsyncMock(return_value=...)`
- Existing 3 pre-existing test failures (not our fault): test_defaults, test_main_without_validate_runs_trading_loop, test_main_without_args_runs_trading_loop
- Baseline: 2274 passed + 3 pre-existing failures

---

## 2026-02-23 - V6-001
- **Implemented**: HistoricalDataProvider — thin wrapper over DataStore for research experiments
- **Files created**:
  - `src/bot/research/data_provider.py` — HistoricalDataProvider class with get_prices, get_ohlcv, get_returns, get_funding_rates, get_multi_prices, get_available_symbols
  - `tests/research/test_data_provider.py` — 20 tests covering all methods, empty data, lookback filtering
- **Files modified**:
  - `src/bot/data/store.py` — Added `get_available_symbols(timeframe, min_count)` async method (DISTINCT symbol with GROUP BY HAVING)
  - `src/bot/research/__init__.py` — Converted to lazy `__getattr__` imports to fix circular dependency; exports HistoricalDataProvider
- **Tests**: 20 new, 2219 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - research/__init__.py MUST use lazy imports — never add eager imports there
  - _CANDLES_PER_DAY mapping handles timeframe→limit conversion (1h=24, 4h=6, 1d=1, etc.)
  - get_multi_prices uses asyncio.gather for parallel fetching
  - Funding rates come 3/day (8h interval), so limit = days × 3

### DataCollector Patterns
- `_dynamic_symbols: set[str]` holds scanner-discovered symbols
- `collect_once()` iterates over `dict.fromkeys(self._symbols + sorted(self._dynamic_symbols))` for dedup
- `bulk_backfill()` uses existing `backfill()` per symbol with 0.5s rate limit between calls
- `TIMEFRAME_SECONDS` dict maps timeframe→seconds; use `86400 // tf_seconds` for candles/day
- `auto_discover_symbols()` takes an OpportunityRegistry and iterates all OpportunityType enum values
- `_backfill_loop()` takes registry + settings as args (not stored), runs every `data_backfill_interval_hours`

### EngineManager Wiring
- `set_collector(collector)` stores DataCollector reference for backfill loop
- `start_background_loops()` starts backfill task if `data_backfill_enabled` and collector is set
- `opportunity_registry` property fetches registry from token_scanner engine
- `start_background_loops()` was NOT called in main.py `_run_engine_mode()` — added it after `start_all()`

---

## 2026-02-23 - V6-002
- **Implemented**: DataCollector batch backfill + scanner auto-discovery
- **Files created**:
  - `tests/data/test_backfill.py` — 22 tests: bulk_backfill, auto_discover, dynamic_symbols merge, backfill_loop, config, EngineManager wiring
- **Files modified**:
  - `src/bot/data/collector.py` — Added `_dynamic_symbols`, `bulk_backfill()`, `auto_discover_symbols()`, `_backfill_loop()`. Modified `collect_once()` to include dynamic symbols
  - `src/bot/config.py` — Added `data_backfill_enabled`, `data_backfill_interval_hours`, `data_backfill_days` settings + SETTINGS_METADATA
  - `src/bot/engines/manager.py` — Added `_collector`, `_backfill_task`, `set_collector()`. Updated `start_background_loops()` to start backfill loop
  - `src/bot/main.py` — Added `set_collector()` call in `_init_engine_mode()`, added `start_background_loops()` call in `_run_engine_mode()`
- **Tests**: 22 new, 2241 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `start_background_loops()` was defined but never called — fixed
  - Rate limit delay between symbols is 0.5s (using `asyncio.sleep`)
  - `dict.fromkeys()` preserves order while deduplicating
  - Backfill loop uses registry from `opportunity_registry` property (finds scanner engine)

### Research Experiment Data Flow
- `ResearchTask` base class stores `data_provider: HistoricalDataProvider | None` (default None for backward compat)
- `_run_async(coro)` helper in ResearchTask: runs async code from sync `run_experiment()` — uses ThreadPoolExecutor when inside running event loop, `asyncio.run()` otherwise
- Data priority in experiments: `kwargs` > `data_provider` > synthetic fallback
- Each experiment adds `data_source: 'real' | 'synthetic' | 'kwargs'` to results dict
- CointegrationExperiment accepts `stat_arb_pairs` list, OptimalGridExperiment accepts `grid_symbols` list
- FundingPrediction: `_real_funding_data` stores raw records for time pattern analysis (hourly/daily)
- Experiments registered in `main.py._register_research_experiments()` with data_provider from DataStore

---

## 2026-02-23 - V6-003
- **Implemented**: Research experiments real data upgrade — all 4 experiments use HistoricalDataProvider
- **Files created**:
  - `tests/research/test_experiments_real_data.py` — 33 tests: real data mode, fallback to synthetic, data_source field, time pattern analysis, backward compatibility
- **Files modified**:
  - `src/bot/research/base.py` — Added `__init__(data_provider=None)` to ResearchTask ABC, `_run_async()` helper for sync→async bridging
  - `src/bot/research/experiments/volatility_regime.py` — Added `data_provider` param, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/cointegration.py` — Added `data_provider` + `stat_arb_pairs` params, `_fetch_real_pairs()`, `data_source` in results
  - `src/bot/research/experiments/optimal_grid.py` — Added `data_provider` + `grid_symbols` params, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/funding_prediction.py` — Added `data_provider` param, `_fetch_real_funding_rates()`, time pattern analysis (`_analyze_time_patterns()`), `data_source`/`best_entry_hour`/`avg_positive_rate`/`hourly_pattern`/`daily_pattern` in results
  - `src/bot/main.py` — Added `_register_research_experiments()` method that creates all 4 experiments with HistoricalDataProvider and registers them on EngineManager
  - `scripts/ralph/prd.json` — Marked V6-003 passes: true
- **Tests**: 33 new, 2274 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_run_async()` pattern: `ThreadPoolExecutor` + `asyncio.run` for running async from sync inside event loop
  - Each experiment's `__init__` must call `super().__init__(data_provider=data_provider)` to store on base
  - Funding rate timestamps can be `datetime` objects or epoch milliseconds (int) — handle both
  - experiments/__init__.py uses eager imports (not lazy) — safe since it doesn't import from engines
  - `data_source` field is always present in results for V6-003+, enabling downstream analysis of data quality

### ResearchDeployer Patterns
- `ResearchDeployer(tuner, settings, tracker)` — takes ParameterTuner, Settings, EngineTracker
- `evaluate_report()` → `DeployDecision(action, reason, changes)` — filters valid changes within TUNER_CONFIG bounds
- `deploy()` → saves pre-deploy param snapshot + Sharpe, applies via `tuner.apply_changes()`, records history
- `check_regression(engine_name)` → True if post-deploy Sharpe dropped >30% from pre-deploy
- `rollback(snapshot_id)` → restores params via `settings.reload(snapshot)`, marks history as rolled_back
- Safety bounds: max 3 changes per deploy, all clamped to TUNER_CONFIG min/max
- `_param_snapshots: dict[str, dict]` keyed by uuid-based snapshot_id
- `_pre_deploy_sharpe: dict[str, float]` keyed by engine_name, cleared on rollback
- EngineManager wiring: `set_deployer()` + `_regression_check_loop()` background task
- `_research_loop()` uses deployer when available, falls back to direct `tuner.apply_changes()`
- Config: `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0`
- Dashboard: `GET /api/research/deployments` returns deployment history

---

## 2026-02-23 - V6-004
- **Implemented**: ResearchDeployer — auto-deploy pipeline with A/B verification + safe rollback
- **Files created**:
  - `src/bot/research/deployer.py` — ResearchDeployer class with DeployDecision, DeployResult, DeployRecord dataclasses. evaluate_report(), deploy(), check_regression(), rollback(), get_deploy_history(), _filter_valid_changes()
  - `tests/research/test_deployer.py` — 40 tests: evaluate (significant/not/no-changes/bounds), deploy (success/snapshot/history/sharpe/tuner), regression (none/stable/detected/negative), rollback (success/not-found/clear-sharpe/failure), history (empty/multiple/limit-50), safety bounds (constant/within/clamped/unknown), EngineManager integration, dashboard endpoint, config settings
- **Files modified**:
  - `src/bot/engines/manager.py` — Added `_deployer`, `_regression_task`, `set_deployer()`, `_regression_check_loop()`. Updated `_research_loop()` to use deployer when available. Updated `start_background_loops()` to start regression check loop
  - `src/bot/config.py` — Added `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0` + SETTINGS_METADATA entries
  - `src/bot/dashboard/app.py` — Added `GET /api/research/deployments` endpoint on research_router
  - `src/bot/main.py` — Added ResearchDeployer creation and `set_deployer()` call in `_init_engine_mode()`
  - `src/bot/research/__init__.py` — Added `ResearchDeployer` to lazy exports
  - `scripts/ralph/prd.json` — Marked V6-004 passes: true
- **Tests**: 40 new, 2314 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - deployer.py imports TUNER_CONFIG for bound validation — safe since tuner.py doesn't import deployer
  - `settings.reload(snapshot)` for rollback — same method used by dashboard settings endpoint
  - `_regression_check_loop` only checks the most recent non-rolled-back deployment to avoid cascading rollbacks
  - Pre-deploy Sharpe is cleared per-engine on rollback to prevent stale regression checks
  - `uuid.uuid4()[:8]` for snapshot IDs — short enough for logging, unique enough for tracking
---

### VolatilityService Patterns
- `VolatilityService(data_provider=None)` → graceful degradation: all forecasts None, all regimes NORMAL
- `fit_symbol()` wraps GARCHModel.fit() in try/except — always returns bool success
- `_models: dict[str, GARCHModel]` stores fitted models; `_forecasts: dict[str, float]` caches horizon=1 forecast
- `_regimes: dict[str, VolatilityRegime]` caches classify_volatility_regime() result
- `_last_fit: dict[str, datetime]` tracks fit timestamps for `needs_refit()` staleness check
- `get_market_regime()` uses BTC/USDT as proxy — NORMAL if BTC not fitted
- `_fit_loop()` has 60s initial delay, then runs fit_all() every interval_hours
- Import asyncio at module level (not inside method) — needed for `patch()` in tests

---

## 2026-02-23 - V6-005
- **Implemented**: VolatilityService — GARCH-based real-time volatility forecasting service
- **Files created**:
  - `src/bot/risk/volatility_service.py` — VolatilityService class with fit_symbol, fit_all, get_forecast, get_regime, get_all_regimes, get_market_regime, needs_refit, get_model, _fit_loop
  - `tests/risk/test_volatility_service.py` — 43 tests: init, fit_symbol (success/no-provider/insufficient/empty/garch-failure/exception/timestamps), fit_all (success/partial-failure/empty), get_forecast (after-fit/not-fitted/unknown/horizon>1/no-model), get_regime (after-fit/not-fitted/unknown/all/empty), market_regime (btc/no-btc/only-eth), needs_refit (never/just-fitted/expired/not-expired/custom-age), get_model (after-fit/not-fitted), graceful degradation (6 tests), fit_loop (execution/exception-handling), import verification
- **Files modified**:
  - `src/bot/risk/__init__.py` — Added VolatilityService to exports
  - `scripts/ralph/prd.json` — Marked V6-005 passes: true
- **Tests**: 43 new, 2357 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - asyncio must be imported at module level to be patchable in tests (not inside method)
  - GARCHModel.fit() requires ≥30 data points; returns `{'success': False}` on failure
  - classify_volatility_regime() needs ≥2×window data points, returns NORMAL otherwise
  - get_all_regimes() returns a copy (dict()) to prevent external mutation
  - _fit_loop test pattern: use CancelledError from mock sleep to break after one iteration
---
