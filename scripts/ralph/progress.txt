# Ralph Progress Log
Started: 2026년 2월 23일 월요일 17시 18분 03초 KST
---

## Codebase Patterns

### Circular Import: research ↔ engines
- `bot.research.base` → `bot.engines.tuner` (ParamChange) → `bot.engines.__init__` → `bot.engines.manager` → `bot.research.base` (circular!)
- Fix: `bot.research/__init__.py` uses lazy `__getattr__` imports instead of module-level imports
- Existing tests work only because they import engines before research (by coincidence)
- Any new module in research/ can be imported directly (`from bot.research.data_provider import X`) without triggering the circular chain

### DataStore Patterns
- `get_candles()` returns `list[OHLCV]` (Pydantic models), sorted oldest→newest
- `get_funding_rates()` returns `list[dict]` with keys: symbol, timestamp, funding_rate, funding_timestamp, mark_price, spot_price, spread_pct
- New query methods: just add async methods using `self._session_factory()` context manager
- `func.count()` from sqlalchemy for aggregate queries

### Testing Patterns
- Use `MagicMock()` for DataStore, set async methods via `AsyncMock(return_value=...)`
- Existing 3 pre-existing test failures (not our fault): test_defaults, test_main_without_validate_runs_trading_loop, test_main_without_args_runs_trading_loop
- Baseline: 2274 passed + 3 pre-existing failures

---

## 2026-02-23 - V6-001
- **Implemented**: HistoricalDataProvider — thin wrapper over DataStore for research experiments
- **Files created**:
  - `src/bot/research/data_provider.py` — HistoricalDataProvider class with get_prices, get_ohlcv, get_returns, get_funding_rates, get_multi_prices, get_available_symbols
  - `tests/research/test_data_provider.py` — 20 tests covering all methods, empty data, lookback filtering
- **Files modified**:
  - `src/bot/data/store.py` — Added `get_available_symbols(timeframe, min_count)` async method (DISTINCT symbol with GROUP BY HAVING)
  - `src/bot/research/__init__.py` — Converted to lazy `__getattr__` imports to fix circular dependency; exports HistoricalDataProvider
- **Tests**: 20 new, 2219 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - research/__init__.py MUST use lazy imports — never add eager imports there
  - _CANDLES_PER_DAY mapping handles timeframe→limit conversion (1h=24, 4h=6, 1d=1, etc.)
  - get_multi_prices uses asyncio.gather for parallel fetching
  - Funding rates come 3/day (8h interval), so limit = days × 3

### DataCollector Patterns
- `_dynamic_symbols: set[str]` holds scanner-discovered symbols
- `collect_once()` iterates over `dict.fromkeys(self._symbols + sorted(self._dynamic_symbols))` for dedup
- `bulk_backfill()` uses existing `backfill()` per symbol with 0.5s rate limit between calls
- `TIMEFRAME_SECONDS` dict maps timeframe→seconds; use `86400 // tf_seconds` for candles/day
- `auto_discover_symbols()` takes an OpportunityRegistry and iterates all OpportunityType enum values
- `_backfill_loop()` takes registry + settings as args (not stored), runs every `data_backfill_interval_hours`

### EngineManager Wiring
- `set_collector(collector)` stores DataCollector reference for backfill loop
- `start_background_loops()` starts backfill task if `data_backfill_enabled` and collector is set
- `opportunity_registry` property fetches registry from token_scanner engine
- `start_background_loops()` was NOT called in main.py `_run_engine_mode()` — added it after `start_all()`

---

## 2026-02-23 - V6-002
- **Implemented**: DataCollector batch backfill + scanner auto-discovery
- **Files created**:
  - `tests/data/test_backfill.py` — 22 tests: bulk_backfill, auto_discover, dynamic_symbols merge, backfill_loop, config, EngineManager wiring
- **Files modified**:
  - `src/bot/data/collector.py` — Added `_dynamic_symbols`, `bulk_backfill()`, `auto_discover_symbols()`, `_backfill_loop()`. Modified `collect_once()` to include dynamic symbols
  - `src/bot/config.py` — Added `data_backfill_enabled`, `data_backfill_interval_hours`, `data_backfill_days` settings + SETTINGS_METADATA
  - `src/bot/engines/manager.py` — Added `_collector`, `_backfill_task`, `set_collector()`. Updated `start_background_loops()` to start backfill loop
  - `src/bot/main.py` — Added `set_collector()` call in `_init_engine_mode()`, added `start_background_loops()` call in `_run_engine_mode()`
- **Tests**: 22 new, 2241 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `start_background_loops()` was defined but never called — fixed
  - Rate limit delay between symbols is 0.5s (using `asyncio.sleep`)
  - `dict.fromkeys()` preserves order while deduplicating
  - Backfill loop uses registry from `opportunity_registry` property (finds scanner engine)

### Research Experiment Data Flow
- `ResearchTask` base class stores `data_provider: HistoricalDataProvider | None` (default None for backward compat)
- `_run_async(coro)` helper in ResearchTask: runs async code from sync `run_experiment()` — uses ThreadPoolExecutor when inside running event loop, `asyncio.run()` otherwise
- Data priority in experiments: `kwargs` > `data_provider` > synthetic fallback
- Each experiment adds `data_source: 'real' | 'synthetic' | 'kwargs'` to results dict
- CointegrationExperiment accepts `stat_arb_pairs` list, OptimalGridExperiment accepts `grid_symbols` list
- FundingPrediction: `_real_funding_data` stores raw records for time pattern analysis (hourly/daily)
- Experiments registered in `main.py._register_research_experiments()` with data_provider from DataStore

---

## 2026-02-23 - V6-003
- **Implemented**: Research experiments real data upgrade — all 4 experiments use HistoricalDataProvider
- **Files created**:
  - `tests/research/test_experiments_real_data.py` — 33 tests: real data mode, fallback to synthetic, data_source field, time pattern analysis, backward compatibility
- **Files modified**:
  - `src/bot/research/base.py` — Added `__init__(data_provider=None)` to ResearchTask ABC, `_run_async()` helper for sync→async bridging
  - `src/bot/research/experiments/volatility_regime.py` — Added `data_provider` param, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/cointegration.py` — Added `data_provider` + `stat_arb_pairs` params, `_fetch_real_pairs()`, `data_source` in results
  - `src/bot/research/experiments/optimal_grid.py` — Added `data_provider` + `grid_symbols` params, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/funding_prediction.py` — Added `data_provider` param, `_fetch_real_funding_rates()`, time pattern analysis (`_analyze_time_patterns()`), `data_source`/`best_entry_hour`/`avg_positive_rate`/`hourly_pattern`/`daily_pattern` in results
  - `src/bot/main.py` — Added `_register_research_experiments()` method that creates all 4 experiments with HistoricalDataProvider and registers them on EngineManager
  - `scripts/ralph/prd.json` — Marked V6-003 passes: true
- **Tests**: 33 new, 2274 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_run_async()` pattern: `ThreadPoolExecutor` + `asyncio.run` for running async from sync inside event loop
  - Each experiment's `__init__` must call `super().__init__(data_provider=data_provider)` to store on base
  - Funding rate timestamps can be `datetime` objects or epoch milliseconds (int) — handle both
  - experiments/__init__.py uses eager imports (not lazy) — safe since it doesn't import from engines
  - `data_source` field is always present in results for V6-003+, enabling downstream analysis of data quality

### ResearchDeployer Patterns
- `ResearchDeployer(tuner, settings, tracker)` — takes ParameterTuner, Settings, EngineTracker
- `evaluate_report()` → `DeployDecision(action, reason, changes)` — filters valid changes within TUNER_CONFIG bounds
- `deploy()` → saves pre-deploy param snapshot + Sharpe, applies via `tuner.apply_changes()`, records history
- `check_regression(engine_name)` → True if post-deploy Sharpe dropped >30% from pre-deploy
- `rollback(snapshot_id)` → restores params via `settings.reload(snapshot)`, marks history as rolled_back
- Safety bounds: max 3 changes per deploy, all clamped to TUNER_CONFIG min/max
- `_param_snapshots: dict[str, dict]` keyed by uuid-based snapshot_id
- `_pre_deploy_sharpe: dict[str, float]` keyed by engine_name, cleared on rollback
- EngineManager wiring: `set_deployer()` + `_regression_check_loop()` background task
- `_research_loop()` uses deployer when available, falls back to direct `tuner.apply_changes()`
- Config: `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0`
- Dashboard: `GET /api/research/deployments` returns deployment history

---

## 2026-02-23 - V6-004
- **Implemented**: ResearchDeployer — auto-deploy pipeline with A/B verification + safe rollback
- **Files created**:
  - `src/bot/research/deployer.py` — ResearchDeployer class with DeployDecision, DeployResult, DeployRecord dataclasses. evaluate_report(), deploy(), check_regression(), rollback(), get_deploy_history(), _filter_valid_changes()
  - `tests/research/test_deployer.py` — 40 tests: evaluate (significant/not/no-changes/bounds), deploy (success/snapshot/history/sharpe/tuner), regression (none/stable/detected/negative), rollback (success/not-found/clear-sharpe/failure), history (empty/multiple/limit-50), safety bounds (constant/within/clamped/unknown), EngineManager integration, dashboard endpoint, config settings
- **Files modified**:
  - `src/bot/engines/manager.py` — Added `_deployer`, `_regression_task`, `set_deployer()`, `_regression_check_loop()`. Updated `_research_loop()` to use deployer when available. Updated `start_background_loops()` to start regression check loop
  - `src/bot/config.py` — Added `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0` + SETTINGS_METADATA entries
  - `src/bot/dashboard/app.py` — Added `GET /api/research/deployments` endpoint on research_router
  - `src/bot/main.py` — Added ResearchDeployer creation and `set_deployer()` call in `_init_engine_mode()`
  - `src/bot/research/__init__.py` — Added `ResearchDeployer` to lazy exports
  - `scripts/ralph/prd.json` — Marked V6-004 passes: true
- **Tests**: 40 new, 2314 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - deployer.py imports TUNER_CONFIG for bound validation — safe since tuner.py doesn't import deployer
  - `settings.reload(snapshot)` for rollback — same method used by dashboard settings endpoint
  - `_regression_check_loop` only checks the most recent non-rolled-back deployment to avoid cascading rollbacks
  - Pre-deploy Sharpe is cleared per-engine on rollback to prevent stale regression checks
  - `uuid.uuid4()[:8]` for snapshot IDs — short enough for logging, unique enough for tracking
---

### VolatilityService Patterns
- `VolatilityService(data_provider=None)` → graceful degradation: all forecasts None, all regimes NORMAL
- `fit_symbol()` wraps GARCHModel.fit() in try/except — always returns bool success
- `_models: dict[str, GARCHModel]` stores fitted models; `_forecasts: dict[str, float]` caches horizon=1 forecast
- `_regimes: dict[str, VolatilityRegime]` caches classify_volatility_regime() result
- `_last_fit: dict[str, datetime]` tracks fit timestamps for `needs_refit()` staleness check
- `get_market_regime()` uses BTC/USDT as proxy — NORMAL if BTC not fitted
- `_fit_loop()` has 60s initial delay, then runs fit_all() every interval_hours
- Import asyncio at module level (not inside method) — needed for `patch()` in tests

---

## 2026-02-23 - V6-005
- **Implemented**: VolatilityService — GARCH-based real-time volatility forecasting service
- **Files created**:
  - `src/bot/risk/volatility_service.py` — VolatilityService class with fit_symbol, fit_all, get_forecast, get_regime, get_all_regimes, get_market_regime, needs_refit, get_model, _fit_loop
  - `tests/risk/test_volatility_service.py` — 43 tests: init, fit_symbol (success/no-provider/insufficient/empty/garch-failure/exception/timestamps), fit_all (success/partial-failure/empty), get_forecast (after-fit/not-fitted/unknown/horizon>1/no-model), get_regime (after-fit/not-fitted/unknown/all/empty), market_regime (btc/no-btc/only-eth), needs_refit (never/just-fitted/expired/not-expired/custom-age), get_model (after-fit/not-fitted), graceful degradation (6 tests), fit_loop (execution/exception-handling), import verification
- **Files modified**:
  - `src/bot/risk/__init__.py` — Added VolatilityService to exports
  - `scripts/ralph/prd.json` — Marked V6-005 passes: true
- **Tests**: 43 new, 2357 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - asyncio must be imported at module level to be patchable in tests (not inside method)
  - GARCHModel.fit() requires ≥30 data points; returns `{'success': False}` on failure
  - classify_volatility_regime() needs ≥2×window data points, returns NORMAL otherwise
  - get_all_regimes() returns a copy (dict()) to prevent external mutation
  - _fit_loop test pattern: use CancelledError from mock sleep to break after one iteration

### PortfolioRiskManager VaR/CVaR Patterns
- `PortfolioRiskManager(volatility_service=None)` — new optional param, backward compatible
- `_get_portfolio_returns()` — shared helper extracts weighted portfolio returns from `_price_history` and `_positions`
- `calculate_parametric_var()`, `calculate_cornish_fisher_var()`, `calculate_cvar()` — delegate to `quant.risk_metrics` functions, return percentage or None
- `calculate_stress_var(n_simulations=1000)` — Monte Carlo with Cholesky decomposition for correlated returns, uses `np.random.default_rng(42)` for determinism
- Stress VaR handles: single-asset (no correlation), NaN in corr_matrix (`nan_to_num`), non-PSD matrix (eigenvalue clipping), Cholesky failure (fallback to identity)
- `pre_trade_var_check(symbol, position_value)` — simulates adding new position to portfolio, checks VaR against limit
- `validate_new_position()` now has 6 checks: exposure → correlation → sector → heat → VaR limit → pre-trade VaR
- `get_risk_metrics()` includes: parametric_var, cornish_fisher_var, cvar, stress_var (all new fields)
- Config: `var_method: str = 'historical'`, `stress_var_simulations: int = 1000`
- Dashboard: `GET /api/risk/portfolio` returns full risk metrics + position details
- `_get_portfolio_risk_manager()` helper in dashboard finds PRM from `_engine_manager._portfolio_risk`

---

## 2026-02-23 - V6-006
- **Implemented**: VaR/CVaR portfolio risk limits — real-time risk gate with parametric, Cornish-Fisher, stress VaR
- **Files created**:
  - `tests/risk/test_var_integration.py` — 47 tests: _get_portfolio_returns (5), parametric_var (4), cornish_fisher_var (4), cvar (3), stress_var (9), pre_trade_var_check (5), get_risk_metrics format (5), validate_with_pre_trade_var (2), backward_compat (4), config (4), dashboard endpoint (2)
- **Files modified**:
  - `src/bot/risk/portfolio_risk.py` — Added volatility_service param, _get_portfolio_returns(), calculate_parametric_var(), calculate_cornish_fisher_var(), calculate_cvar(), calculate_stress_var(), pre_trade_var_check(). Enhanced get_risk_metrics() with 4 new VaR fields. Added pre_trade_var_check to validate_new_position() chain.
  - `src/bot/config.py` — Added `var_method: str = 'historical'`, `stress_var_simulations: int = 1000` + SETTINGS_METADATA entries
  - `src/bot/dashboard/app.py` — Added risk_router with `GET /api/risk/portfolio` endpoint
  - `scripts/ralph/prd.json` — Marked V6-006 passes: true
- **Tests**: 47 new, 2404 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_get_portfolio_returns()` factored out to avoid code duplication across VaR methods
  - Stress VaR uses `np.random.default_rng(42)` for deterministic results (testable)
  - Cholesky decomposition needs PSD matrix — use eigenvalue clipping as fallback
  - `from scipy import stats as scipy_stats` was unused — ruff caught it immediately
  - Import ordering: ruff wants `from unittest.mock` (stdlib) before `import numpy` (third-party) — use `ruff --fix` to auto-sort
  - pre_trade_var_check creates hypothetical portfolio with new position weight included in total value
  - Dashboard helper `_get_portfolio_risk_manager()` uses `getattr()` for safe access to `_engine_manager._portfolio_risk`

### DynamicPositionSizer Patterns
- `DynamicPositionSizer(volatility_service=None, portfolio_risk=None, base_risk_pct=1.0, vol_scale_factor=1.0)`
- `calculate_size(symbol, price, portfolio_value, atr=None) -> PositionSize`
- Sizing priority: GARCH (median_cond_vol / forecast) → ATR (target_atr / actual_atr_pct) → fixed (1.0)
- vol_multiplier always clamped to [0.25, 2.0]
- `validate_size(position_size, portfolio_value, max_pct) -> PositionSize` — clips notional to max_pct of portfolio
- BaseEngine has `_dynamic_sizer: Any | None = None` + `set_sizer(sizer)` method
- When `_dynamic_sizer is None` → all engines use existing fixed sizing (100% backward compat)
- Each engine imports `PositionSize` inside the method body (lazy) to avoid circular imports
- DecisionStep for sizing: label="포지션 사이징", category="evaluate"
- Config: `dynamic_sizing_enabled`, `vol_scale_factor`, `max_position_scale` (section: Risk Management)
- main.py wires sizer after deployer, iterates engines with `set_sizer()` (excludes token_scanner)

---

## 2026-02-23 - V6-007
- **Implemented**: DynamicPositionSizer — GARCH + ATR integrated volatility-based dynamic position sizing
- **Files created**:
  - `src/bot/risk/dynamic_sizer.py` — DynamicPositionSizer class with PositionSize dataclass, calculate_size (GARCH/ATR/fixed fallback), validate_size (clipping)
  - `tests/risk/test_dynamic_sizer.py` — 39 tests: PositionSize dataclass, init, fixed sizing (5), GARCH sizing (10: high/low vol, clamped bounds, zero/None forecast, no model/cond_vol, zero median), ATR sizing (6: high/low/moderate vol, zero/negative, GARCH precedence), validate_size (6: within/exceeds/zero/boundary), vol_multiplier bounds (2), DecisionStep (1), BaseEngine integration (2), config (3), imports (2)
- **Files modified**:
  - `src/bot/engines/base.py` — Added `_dynamic_sizer: Any | None = None` field, `set_sizer()` method
  - `src/bot/engines/funding_arb.py` — `_open_position()` uses dynamic sizer when available, DecisionStep for sizing in `_run_cycle()`
  - `src/bot/engines/grid_trading.py` — `_check_fills()` uses dynamic sizer for notional_qty, DecisionStep for sizing on grid init
  - `src/bot/engines/cross_exchange_arb.py` — `_execute_arb()` uses dynamic sizer for quantity, DecisionStep for sizing, fixed notional calc to use actual quantity
  - `src/bot/engines/stat_arb.py` — `_check_entry()` uses dynamic sizer to split notional across pair legs, DecisionStep for sizing
  - `src/bot/config.py` — Added `dynamic_sizing_enabled: bool = True`, `vol_scale_factor: float = 1.0`, `max_position_scale: float = 2.0` + SETTINGS_METADATA entries
  - `src/bot/main.py` — Added DynamicPositionSizer creation and `set_sizer()` wiring in `_init_engine_mode()`
  - `src/bot/risk/__init__.py` — Added DynamicPositionSizer, PositionSize to exports
  - `scripts/ralph/prd.json` — Marked V6-007 passes: true
- **Tests**: 39 new, 2443 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `Any` type hint for `_dynamic_sizer` in BaseEngine avoids importing from risk package (no circular dep)
  - Lazy `from bot.risk.dynamic_sizer import PositionSize` inside engine methods — safe since dynamic_sizer doesn't import engines
  - GARCH median_vol from `model.conditional_volatility` (np.ndarray) via `np.median()`
  - ATR is passed as absolute value, converted to fraction of price internally: `atr_pct = atr / price`
  - cross_exchange_arb's notional now uses `quantity * buy_price` (actual) instead of `_max_position_per_symbol` when dynamic sizer is active
  - stat_arb splits total `notional_value / 2` across pair legs
---
