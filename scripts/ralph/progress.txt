# Ralph Progress Log
Started: 2026년 2월 23일 월요일 17시 18분 03초 KST
---

## Codebase Patterns

### Circular Import: research ↔ engines
- `bot.research.base` → `bot.engines.tuner` (ParamChange) → `bot.engines.__init__` → `bot.engines.manager` → `bot.research.base` (circular!)
- Fix: `bot.research/__init__.py` uses lazy `__getattr__` imports instead of module-level imports
- Existing tests work only because they import engines before research (by coincidence)
- Any new module in research/ can be imported directly (`from bot.research.data_provider import X`) without triggering the circular chain

### DataStore Patterns
- `get_candles()` returns `list[OHLCV]` (Pydantic models), sorted oldest→newest
- `get_funding_rates()` returns `list[dict]` with keys: symbol, timestamp, funding_rate, funding_timestamp, mark_price, spot_price, spread_pct
- New query methods: just add async methods using `self._session_factory()` context manager
- `func.count()` from sqlalchemy for aggregate queries

### Testing Patterns
- Use `MagicMock()` for DataStore, set async methods via `AsyncMock(return_value=...)`
- Existing 3 pre-existing test failures (not our fault): test_defaults, test_main_without_validate_runs_trading_loop, test_main_without_args_runs_trading_loop
- Baseline: 2274 passed + 3 pre-existing failures

---

## 2026-02-23 - V6-001
- **Implemented**: HistoricalDataProvider — thin wrapper over DataStore for research experiments
- **Files created**:
  - `src/bot/research/data_provider.py` — HistoricalDataProvider class with get_prices, get_ohlcv, get_returns, get_funding_rates, get_multi_prices, get_available_symbols
  - `tests/research/test_data_provider.py` — 20 tests covering all methods, empty data, lookback filtering
- **Files modified**:
  - `src/bot/data/store.py` — Added `get_available_symbols(timeframe, min_count)` async method (DISTINCT symbol with GROUP BY HAVING)
  - `src/bot/research/__init__.py` — Converted to lazy `__getattr__` imports to fix circular dependency; exports HistoricalDataProvider
- **Tests**: 20 new, 2219 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - research/__init__.py MUST use lazy imports — never add eager imports there
  - _CANDLES_PER_DAY mapping handles timeframe→limit conversion (1h=24, 4h=6, 1d=1, etc.)
  - get_multi_prices uses asyncio.gather for parallel fetching
  - Funding rates come 3/day (8h interval), so limit = days × 3

### DataCollector Patterns
- `_dynamic_symbols: set[str]` holds scanner-discovered symbols
- `collect_once()` iterates over `dict.fromkeys(self._symbols + sorted(self._dynamic_symbols))` for dedup
- `bulk_backfill()` uses existing `backfill()` per symbol with 0.5s rate limit between calls
- `TIMEFRAME_SECONDS` dict maps timeframe→seconds; use `86400 // tf_seconds` for candles/day
- `auto_discover_symbols()` takes an OpportunityRegistry and iterates all OpportunityType enum values
- `_backfill_loop()` takes registry + settings as args (not stored), runs every `data_backfill_interval_hours`

### EngineManager Wiring
- `set_collector(collector)` stores DataCollector reference for backfill loop
- `start_background_loops()` starts backfill task if `data_backfill_enabled` and collector is set
- `opportunity_registry` property fetches registry from token_scanner engine
- `start_background_loops()` was NOT called in main.py `_run_engine_mode()` — added it after `start_all()`

---

## 2026-02-23 - V6-002
- **Implemented**: DataCollector batch backfill + scanner auto-discovery
- **Files created**:
  - `tests/data/test_backfill.py` — 22 tests: bulk_backfill, auto_discover, dynamic_symbols merge, backfill_loop, config, EngineManager wiring
- **Files modified**:
  - `src/bot/data/collector.py` — Added `_dynamic_symbols`, `bulk_backfill()`, `auto_discover_symbols()`, `_backfill_loop()`. Modified `collect_once()` to include dynamic symbols
  - `src/bot/config.py` — Added `data_backfill_enabled`, `data_backfill_interval_hours`, `data_backfill_days` settings + SETTINGS_METADATA
  - `src/bot/engines/manager.py` — Added `_collector`, `_backfill_task`, `set_collector()`. Updated `start_background_loops()` to start backfill loop
  - `src/bot/main.py` — Added `set_collector()` call in `_init_engine_mode()`, added `start_background_loops()` call in `_run_engine_mode()`
- **Tests**: 22 new, 2241 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `start_background_loops()` was defined but never called — fixed
  - Rate limit delay between symbols is 0.5s (using `asyncio.sleep`)
  - `dict.fromkeys()` preserves order while deduplicating
  - Backfill loop uses registry from `opportunity_registry` property (finds scanner engine)

### Research Experiment Data Flow
- `ResearchTask` base class stores `data_provider: HistoricalDataProvider | None` (default None for backward compat)
- `_run_async(coro)` helper in ResearchTask: runs async code from sync `run_experiment()` — uses ThreadPoolExecutor when inside running event loop, `asyncio.run()` otherwise
- Data priority in experiments: `kwargs` > `data_provider` > synthetic fallback
- Each experiment adds `data_source: 'real' | 'synthetic' | 'kwargs'` to results dict
- CointegrationExperiment accepts `stat_arb_pairs` list, OptimalGridExperiment accepts `grid_symbols` list
- FundingPrediction: `_real_funding_data` stores raw records for time pattern analysis (hourly/daily)
- Experiments registered in `main.py._register_research_experiments()` with data_provider from DataStore

---

## 2026-02-23 - V6-003
- **Implemented**: Research experiments real data upgrade — all 4 experiments use HistoricalDataProvider
- **Files created**:
  - `tests/research/test_experiments_real_data.py` — 33 tests: real data mode, fallback to synthetic, data_source field, time pattern analysis, backward compatibility
- **Files modified**:
  - `src/bot/research/base.py` — Added `__init__(data_provider=None)` to ResearchTask ABC, `_run_async()` helper for sync→async bridging
  - `src/bot/research/experiments/volatility_regime.py` — Added `data_provider` param, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/cointegration.py` — Added `data_provider` + `stat_arb_pairs` params, `_fetch_real_pairs()`, `data_source` in results
  - `src/bot/research/experiments/optimal_grid.py` — Added `data_provider` + `grid_symbols` params, `_fetch_real_prices()`, `data_source` in results
  - `src/bot/research/experiments/funding_prediction.py` — Added `data_provider` param, `_fetch_real_funding_rates()`, time pattern analysis (`_analyze_time_patterns()`), `data_source`/`best_entry_hour`/`avg_positive_rate`/`hourly_pattern`/`daily_pattern` in results
  - `src/bot/main.py` — Added `_register_research_experiments()` method that creates all 4 experiments with HistoricalDataProvider and registers them on EngineManager
  - `scripts/ralph/prd.json` — Marked V6-003 passes: true
- **Tests**: 33 new, 2274 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_run_async()` pattern: `ThreadPoolExecutor` + `asyncio.run` for running async from sync inside event loop
  - Each experiment's `__init__` must call `super().__init__(data_provider=data_provider)` to store on base
  - Funding rate timestamps can be `datetime` objects or epoch milliseconds (int) — handle both
  - experiments/__init__.py uses eager imports (not lazy) — safe since it doesn't import from engines
  - `data_source` field is always present in results for V6-003+, enabling downstream analysis of data quality

### ResearchDeployer Patterns
- `ResearchDeployer(tuner, settings, tracker)` — takes ParameterTuner, Settings, EngineTracker
- `evaluate_report()` → `DeployDecision(action, reason, changes)` — filters valid changes within TUNER_CONFIG bounds
- `deploy()` → saves pre-deploy param snapshot + Sharpe, applies via `tuner.apply_changes()`, records history
- `check_regression(engine_name)` → True if post-deploy Sharpe dropped >30% from pre-deploy
- `rollback(snapshot_id)` → restores params via `settings.reload(snapshot)`, marks history as rolled_back
- Safety bounds: max 3 changes per deploy, all clamped to TUNER_CONFIG min/max
- `_param_snapshots: dict[str, dict]` keyed by uuid-based snapshot_id
- `_pre_deploy_sharpe: dict[str, float]` keyed by engine_name, cleared on rollback
- EngineManager wiring: `set_deployer()` + `_regression_check_loop()` background task
- `_research_loop()` uses deployer when available, falls back to direct `tuner.apply_changes()`
- Config: `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0`
- Dashboard: `GET /api/research/deployments` returns deployment history

---

## 2026-02-23 - V6-004
- **Implemented**: ResearchDeployer — auto-deploy pipeline with A/B verification + safe rollback
- **Files created**:
  - `src/bot/research/deployer.py` — ResearchDeployer class with DeployDecision, DeployResult, DeployRecord dataclasses. evaluate_report(), deploy(), check_regression(), rollback(), get_deploy_history(), _filter_valid_changes()
  - `tests/research/test_deployer.py` — 40 tests: evaluate (significant/not/no-changes/bounds), deploy (success/snapshot/history/sharpe/tuner), regression (none/stable/detected/negative), rollback (success/not-found/clear-sharpe/failure), history (empty/multiple/limit-50), safety bounds (constant/within/clamped/unknown), EngineManager integration, dashboard endpoint, config settings
- **Files modified**:
  - `src/bot/engines/manager.py` — Added `_deployer`, `_regression_task`, `set_deployer()`, `_regression_check_loop()`. Updated `_research_loop()` to use deployer when available. Updated `start_background_loops()` to start regression check loop
  - `src/bot/config.py` — Added `research_auto_deploy: bool = True`, `research_regression_check_hours: float = 6.0` + SETTINGS_METADATA entries
  - `src/bot/dashboard/app.py` — Added `GET /api/research/deployments` endpoint on research_router
  - `src/bot/main.py` — Added ResearchDeployer creation and `set_deployer()` call in `_init_engine_mode()`
  - `src/bot/research/__init__.py` — Added `ResearchDeployer` to lazy exports
  - `scripts/ralph/prd.json` — Marked V6-004 passes: true
- **Tests**: 40 new, 2314 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - deployer.py imports TUNER_CONFIG for bound validation — safe since tuner.py doesn't import deployer
  - `settings.reload(snapshot)` for rollback — same method used by dashboard settings endpoint
  - `_regression_check_loop` only checks the most recent non-rolled-back deployment to avoid cascading rollbacks
  - Pre-deploy Sharpe is cleared per-engine on rollback to prevent stale regression checks
  - `uuid.uuid4()[:8]` for snapshot IDs — short enough for logging, unique enough for tracking
---

### VolatilityService Patterns
- `VolatilityService(data_provider=None)` → graceful degradation: all forecasts None, all regimes NORMAL
- `fit_symbol()` wraps GARCHModel.fit() in try/except — always returns bool success
- `_models: dict[str, GARCHModel]` stores fitted models; `_forecasts: dict[str, float]` caches horizon=1 forecast
- `_regimes: dict[str, VolatilityRegime]` caches classify_volatility_regime() result
- `_last_fit: dict[str, datetime]` tracks fit timestamps for `needs_refit()` staleness check
- `get_market_regime()` uses BTC/USDT as proxy — NORMAL if BTC not fitted
- `_fit_loop()` has 60s initial delay, then runs fit_all() every interval_hours
- Import asyncio at module level (not inside method) — needed for `patch()` in tests

---

## 2026-02-23 - V6-005
- **Implemented**: VolatilityService — GARCH-based real-time volatility forecasting service
- **Files created**:
  - `src/bot/risk/volatility_service.py` — VolatilityService class with fit_symbol, fit_all, get_forecast, get_regime, get_all_regimes, get_market_regime, needs_refit, get_model, _fit_loop
  - `tests/risk/test_volatility_service.py` — 43 tests: init, fit_symbol (success/no-provider/insufficient/empty/garch-failure/exception/timestamps), fit_all (success/partial-failure/empty), get_forecast (after-fit/not-fitted/unknown/horizon>1/no-model), get_regime (after-fit/not-fitted/unknown/all/empty), market_regime (btc/no-btc/only-eth), needs_refit (never/just-fitted/expired/not-expired/custom-age), get_model (after-fit/not-fitted), graceful degradation (6 tests), fit_loop (execution/exception-handling), import verification
- **Files modified**:
  - `src/bot/risk/__init__.py` — Added VolatilityService to exports
  - `scripts/ralph/prd.json` — Marked V6-005 passes: true
- **Tests**: 43 new, 2357 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - asyncio must be imported at module level to be patchable in tests (not inside method)
  - GARCHModel.fit() requires ≥30 data points; returns `{'success': False}` on failure
  - classify_volatility_regime() needs ≥2×window data points, returns NORMAL otherwise
  - get_all_regimes() returns a copy (dict()) to prevent external mutation
  - _fit_loop test pattern: use CancelledError from mock sleep to break after one iteration

### PortfolioRiskManager VaR/CVaR Patterns
- `PortfolioRiskManager(volatility_service=None)` — new optional param, backward compatible
- `_get_portfolio_returns()` — shared helper extracts weighted portfolio returns from `_price_history` and `_positions`
- `calculate_parametric_var()`, `calculate_cornish_fisher_var()`, `calculate_cvar()` — delegate to `quant.risk_metrics` functions, return percentage or None
- `calculate_stress_var(n_simulations=1000)` — Monte Carlo with Cholesky decomposition for correlated returns, uses `np.random.default_rng(42)` for determinism
- Stress VaR handles: single-asset (no correlation), NaN in corr_matrix (`nan_to_num`), non-PSD matrix (eigenvalue clipping), Cholesky failure (fallback to identity)
- `pre_trade_var_check(symbol, position_value)` — simulates adding new position to portfolio, checks VaR against limit
- `validate_new_position()` now has 6 checks: exposure → correlation → sector → heat → VaR limit → pre-trade VaR
- `get_risk_metrics()` includes: parametric_var, cornish_fisher_var, cvar, stress_var (all new fields)
- Config: `var_method: str = 'historical'`, `stress_var_simulations: int = 1000`
- Dashboard: `GET /api/risk/portfolio` returns full risk metrics + position details
- `_get_portfolio_risk_manager()` helper in dashboard finds PRM from `_engine_manager._portfolio_risk`

---

## 2026-02-23 - V6-006
- **Implemented**: VaR/CVaR portfolio risk limits — real-time risk gate with parametric, Cornish-Fisher, stress VaR
- **Files created**:
  - `tests/risk/test_var_integration.py` — 47 tests: _get_portfolio_returns (5), parametric_var (4), cornish_fisher_var (4), cvar (3), stress_var (9), pre_trade_var_check (5), get_risk_metrics format (5), validate_with_pre_trade_var (2), backward_compat (4), config (4), dashboard endpoint (2)
- **Files modified**:
  - `src/bot/risk/portfolio_risk.py` — Added volatility_service param, _get_portfolio_returns(), calculate_parametric_var(), calculate_cornish_fisher_var(), calculate_cvar(), calculate_stress_var(), pre_trade_var_check(). Enhanced get_risk_metrics() with 4 new VaR fields. Added pre_trade_var_check to validate_new_position() chain.
  - `src/bot/config.py` — Added `var_method: str = 'historical'`, `stress_var_simulations: int = 1000` + SETTINGS_METADATA entries
  - `src/bot/dashboard/app.py` — Added risk_router with `GET /api/risk/portfolio` endpoint
  - `scripts/ralph/prd.json` — Marked V6-006 passes: true
- **Tests**: 47 new, 2404 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_get_portfolio_returns()` factored out to avoid code duplication across VaR methods
  - Stress VaR uses `np.random.default_rng(42)` for deterministic results (testable)
  - Cholesky decomposition needs PSD matrix — use eigenvalue clipping as fallback
  - `from scipy import stats as scipy_stats` was unused — ruff caught it immediately
  - Import ordering: ruff wants `from unittest.mock` (stdlib) before `import numpy` (third-party) — use `ruff --fix` to auto-sort
  - pre_trade_var_check creates hypothetical portfolio with new position weight included in total value
  - Dashboard helper `_get_portfolio_risk_manager()` uses `getattr()` for safe access to `_engine_manager._portfolio_risk`

### DynamicPositionSizer Patterns
- `DynamicPositionSizer(volatility_service=None, portfolio_risk=None, base_risk_pct=1.0, vol_scale_factor=1.0)`
- `calculate_size(symbol, price, portfolio_value, atr=None) -> PositionSize`
- Sizing priority: GARCH (median_cond_vol / forecast) → ATR (target_atr / actual_atr_pct) → fixed (1.0)
- vol_multiplier always clamped to [0.25, 2.0]
- `validate_size(position_size, portfolio_value, max_pct) -> PositionSize` — clips notional to max_pct of portfolio
- BaseEngine has `_dynamic_sizer: Any | None = None` + `set_sizer(sizer)` method
- When `_dynamic_sizer is None` → all engines use existing fixed sizing (100% backward compat)
- Each engine imports `PositionSize` inside the method body (lazy) to avoid circular imports
- DecisionStep for sizing: label="포지션 사이징", category="evaluate"
- Config: `dynamic_sizing_enabled`, `vol_scale_factor`, `max_position_scale` (section: Risk Management)
- main.py wires sizer after deployer, iterates engines with `set_sizer()` (excludes token_scanner)

---

## 2026-02-23 - V6-007
- **Implemented**: DynamicPositionSizer — GARCH + ATR integrated volatility-based dynamic position sizing
- **Files created**:
  - `src/bot/risk/dynamic_sizer.py` — DynamicPositionSizer class with PositionSize dataclass, calculate_size (GARCH/ATR/fixed fallback), validate_size (clipping)
  - `tests/risk/test_dynamic_sizer.py` — 39 tests: PositionSize dataclass, init, fixed sizing (5), GARCH sizing (10: high/low vol, clamped bounds, zero/None forecast, no model/cond_vol, zero median), ATR sizing (6: high/low/moderate vol, zero/negative, GARCH precedence), validate_size (6: within/exceeds/zero/boundary), vol_multiplier bounds (2), DecisionStep (1), BaseEngine integration (2), config (3), imports (2)
- **Files modified**:
  - `src/bot/engines/base.py` — Added `_dynamic_sizer: Any | None = None` field, `set_sizer()` method
  - `src/bot/engines/funding_arb.py` — `_open_position()` uses dynamic sizer when available, DecisionStep for sizing in `_run_cycle()`
  - `src/bot/engines/grid_trading.py` — `_check_fills()` uses dynamic sizer for notional_qty, DecisionStep for sizing on grid init
  - `src/bot/engines/cross_exchange_arb.py` — `_execute_arb()` uses dynamic sizer for quantity, DecisionStep for sizing, fixed notional calc to use actual quantity
  - `src/bot/engines/stat_arb.py` — `_check_entry()` uses dynamic sizer to split notional across pair legs, DecisionStep for sizing
  - `src/bot/config.py` — Added `dynamic_sizing_enabled: bool = True`, `vol_scale_factor: float = 1.0`, `max_position_scale: float = 2.0` + SETTINGS_METADATA entries
  - `src/bot/main.py` — Added DynamicPositionSizer creation and `set_sizer()` wiring in `_init_engine_mode()`
  - `src/bot/risk/__init__.py` — Added DynamicPositionSizer, PositionSize to exports
  - `scripts/ralph/prd.json` — Marked V6-007 passes: true
- **Tests**: 39 new, 2443 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `Any` type hint for `_dynamic_sizer` in BaseEngine avoids importing from risk package (no circular dep)
  - Lazy `from bot.risk.dynamic_sizer import PositionSize` inside engine methods — safe since dynamic_sizer doesn't import engines
  - GARCH median_vol from `model.conditional_volatility` (np.ndarray) via `np.median()`
  - ATR is passed as absolute value, converted to fraction of price internally: `atr_pct = atr / price`
  - cross_exchange_arb's notional now uses `quantity * buy_price` (actual) instead of `_max_position_per_symbol` when dynamic sizer is active
  - stat_arb splits total `notional_value / 2` across pair legs

### CorrelationRiskController Patterns
- `CorrelationRiskController(portfolio_risk=None, max_cross_engine_correlation=0.85, max_symbol_concentration=0.4)`
- `update_positions(engine_positions: dict[str, list[dict]])` — full engine→positions map refresh each cycle
- `calculate_cross_engine_correlation()` — symbol-based overlap (not price correlation); returns overlap_pct, concentration_score per engine pair
- `check_symbol_concentration(symbol)` — total notional across all engines vs portfolio value; returns (bool, reason)
- `get_concentration_report()` — per_symbol breakdown, cross_engine_correlations, alerts
- BaseEngine `_has_capacity(symbol=None)` — optional symbol param triggers correlation check if controller is set; no symbol → no check (backward compat)
- EngineManager `_sync_correlation_positions()` — gathers positions from all engines, estimates notional from qty*price if not provided
- Called in `_record_cycle_to_tracker()` after each cycle, and in `_rebalance_loop()` before rebalancing
- Dashboard: `GET /api/risk/correlation` returns concentration report via `getattr(_engine_manager, "_correlation_controller")`
- Config: `cross_engine_correlation_enabled: bool = True`, `max_symbol_concentration_pct: float = 40.0`

---

## 2026-02-23 - V6-008
- **Implemented**: CorrelationRiskController — cross-engine symbol concentration monitoring and limiting
- **Files created**:
  - `src/bot/risk/correlation_controller.py` — CorrelationRiskController class with EnginePosition dataclass, update_positions, calculate_cross_engine_correlation, check_symbol_concentration, get_concentration_report
  - `tests/risk/test_correlation_controller.py` — 40 tests: init (2), update_positions (3), cross_engine_correlation (7: no/single/no_overlap/full/partial/three_engines/empty), check_symbol_concentration (7: within/exceeds/exact/not_present/no_prm/zero_value/engines_in_reason), get_concentration_report (5: empty/structure/concentration_alert/cross_engine_alert/no_portfolio), BaseEngine integration (5: without_ctrl/allowed/blocked/no_symbol_skips/max_positions_precedence), EngineManager integration (4: set_ctrl/sync_positions/estimates_notional/no_ctrl), dashboard endpoint (3: no_manager/no_controller/with_controller), config (2), imports (2)
- **Files modified**:
  - `src/bot/engines/base.py` — Added `_correlation_controller: Any | None = None`, `set_correlation_controller()`, modified `_has_capacity(symbol=None)` with optional cross-engine check
  - `src/bot/engines/manager.py` — Added `_correlation_controller`, `set_correlation_controller()`, `_sync_correlation_positions()`. Updated `_record_cycle_to_tracker()` to sync positions. Updated `_rebalance_loop()` to log concentration alerts
  - `src/bot/engines/funding_arb.py` — Pass symbol to `_has_capacity(symbol)` in `_run_cycle()`
  - `src/bot/engines/stat_arb.py` — Pass pair_key to `_has_capacity(pair_key)` in `_run_cycle()`
  - `src/bot/dashboard/app.py` — Added `GET /api/risk/correlation` endpoint on risk_router
  - `src/bot/config.py` — Added `cross_engine_correlation_enabled: bool = True`, `max_symbol_concentration_pct: float = 40.0` + SETTINGS_METADATA entries
  - `src/bot/main.py` — Added CorrelationRiskController creation and wiring to EngineManager + engines in `_init_engine_mode()`
  - `src/bot/risk/__init__.py` — Added CorrelationRiskController to exports
  - `scripts/ralph/prd.json` — Marked V6-008 passes: true
- **Tests**: 40 new, 2483 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_has_capacity(symbol=None)` default None ensures 100% backward compat — existing callers without args still work
  - Position overlap is symbol-based (Jaccard index), not price-correlation — much cheaper to compute
  - `_sync_correlation_positions()` estimates notional from qty * entry_price when "notional" key is absent
  - `getattr(_engine_manager, "_correlation_controller", None)` pattern for dashboard access — avoids tight coupling
  - Engine tests with MagicMock portfolio manager need `request_capital.return_value` and `get_max_allocation.return_value`
  - pytest-asyncio is needed for async test methods in full suite (event loop issue with `asyncio.get_event_loop()`)
---

### Trade Explorer / Dashboard API Patterns
- `trades_detail_router` uses `require_auth_strict` dependency for auth
- `_collect_tracker_trades()` shared helper gathers from `tracker._trades` (dict of engine→list[TradeRecord])
- Filters: engine (exact match), symbol (exact match), start/end (ISO string comparison on exit_time), win_only (True=wins, False=losses, None=all)
- Results sorted by exit_time descending (newest first), paginated by limit/offset
- CSV export via `fastapi.responses.Response(content=csv, media_type="text/csv")` with Content-Disposition header
- `_format_hold_time(seconds)` → "Xh Ym" or "Ym" format
- Frontend: `apiClient.get('/trades/export', { params, responseType: 'blob' })` + Blob + URL.createObjectURL for download
- TradeExplorer uses client-side sorting (sortField/sortDir) on server-paginated data
- PnL waterfall chart uses recharts BarChart with Cell components for per-bar colors
- `set_engine_manager(mgr)` function allows tests to inject mock EngineManager

---

## 2026-02-23 - V6-009
- **Implemented**: Trade Explorer page — individual trade detail analysis with filtering, PnL breakdown, CSV export
- **Files created**:
  - `frontend/src/pages/TradeExplorer.tsx` — Full Trade Explorer component with trade table (sorting/filtering/pagination), expandable detail panel (PnL waterfall chart, same-symbol history), CSV export
  - `tests/dashboard/test_trade_detail_api.py` — 17 tests: TestTradeDetail (no_manager, all_trades, filter_by_engine, filter_by_symbol, filter_win/loss, pagination, date_range, trade_fields), TestTradeExport (no_manager, csv_format, csv_filter, csv_content_type), TestFormatHoldTime (zero, minutes, hours_and_minutes, negative)
- **Files modified**:
  - `src/bot/dashboard/app.py` — Added trades_detail_router with GET /api/trades/detail (filtering + pagination) and GET /api/trades/export (CSV). Added _collect_tracker_trades() helper and _format_hold_time() helper
  - `frontend/src/api/types.ts` — Added TradeDetail and TradeDetailResponse interfaces
  - `frontend/src/App.tsx` — Added TradeExplorer import, /trade-explorer nav item, and Route
  - `scripts/ralph/prd.json` — Marked V6-009 passes: true
- **Tests**: 17 new, 2500 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `_collect_tracker_trades()` builds trade dicts from TradeRecord dataclass — not Pydantic, so manual dict construction
  - CSV export uses inline `from fastapi.responses import Response` to avoid import pollution
  - Client-side sorting on already-paginated data (20 items per page) is efficient enough
  - PnL waterfall uses negative cost value (`-Math.abs(cost)`) for visual clarity
  - `set_engine_manager()` is essential for test injection — called from tests directly

### Heatmap API Patterns
- `heatmap_router` on `/api/analytics` prefix with `require_auth_strict`
- Single endpoint `GET /api/analytics/heatmap?type=X` dispatches to helper functions
- `_heatmap_hourly_dow()` returns 168 cells (7×24) with pre-populated grid (always returns all cells)
- `_heatmap_engine_symbol()` returns only cells with data (dynamic)
- `_heatmap_monthly()` returns only months with data, sorted chronologically
- All heatmap cells include: pnl, trade_count, win_rate
- Reuses `_collect_tracker_trades()` from trade detail endpoints for data gathering
- Optional `engine` and `days` params for filtering before aggregation
- Frontend uses custom div grid (not recharts) for heatmaps with rgba color scaling
- `pnlColor()` calculates rgba intensity from value/maxAbs ratio
- Tooltip follows mouse via `onMouseEnter`/`onMouseMove`/`onMouseLeave` with fixed positioning
---

## 2026-02-23 - V6-010
- **Implemented**: Performance Heatmap — hourly/DOW PnL, engine×symbol matrix, monthly calendar
- **Files created**:
  - `frontend/src/pages/Heatmaps.tsx` — Three heatmap views: hourly×DOW grid (7×24 cells), engine×symbol matrix, monthly PnL calendar with year navigation. Custom tooltip, color scaling, engine/period filters
  - `tests/dashboard/test_heatmap_api.py` — 18 tests: TestHeatmapHourlyDow (no_manager, grid_size=168, fields, pnl_values, loss_cell, empty_cell), TestHeatmapEngineSymbol (data_count, fields, aggregation, engine_filter), TestHeatmapMonthly (data, fields, chronological_sort, pnl_aggregation), TestHeatmapUnknownType (error), TestHeatmapEmptyTracker (3 types)
- **Files modified**:
  - `src/bot/dashboard/app.py` — Added heatmap_router with GET /api/analytics/heatmap, _heatmap_hourly_dow(), _heatmap_engine_symbol(), _heatmap_monthly() helpers
  - `frontend/src/api/types.ts` — Added HeatmapHourlyDow, HeatmapEngineSymbol, HeatmapMonthly interfaces
  - `frontend/src/App.tsx` — Added Heatmaps import, /heatmaps nav item, and Route
  - `scripts/ralph/prd.json` — Marked V6-010 passes: true
- **Tests**: 18 new, 2518 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - Hourly×DOW grid must pre-populate all 168 cells (empty cells have pnl=0, count=0)
  - `datetime.fromisoformat()` needs "Z" → "+00:00" replacement for Python 3.10
  - `datetime.weekday()` returns 0=Mon..6=Sun (matches DOW_LABELS order)
  - `_collect_tracker_trades()` reuse avoids code duplication between trade detail and heatmap endpoints
  - Frontend rgba color scaling: `rgba(r, g, b, intensity)` where intensity = abs(value)/maxAbs
  - Monthly calendar sorted by (year, month) for chronological display

### Risk Dashboard Patterns
- `PortfolioManager._drawdown_history: list[dict]` appended in `report_pnl()` with {timestamp, drawdown_pct, equity}
- Capped at 1000 entries with slicing
- `GET /api/risk/drawdown` returns `{"history": pm._drawdown_history}` via `getattr(_engine_manager, "_portfolio_manager")`
- Frontend uses `Promise.allSettled()` for parallel fetch of /risk/portfolio, /risk/drawdown, /risk/correlation (graceful degradation)
- VaR gauge uses recharts RadialBarChart with startAngle=180 endAngle=0 for semicircle
- Drawdown curve: AreaChart with reversed Y-axis (drawdown % grows downward)
- Correlation heatmap: 4×4 engine grid with overlap_pct from corrMap
- TypeScript: `(data as any).error` pattern to check API error responses on typed interfaces
- `Tooltip formatter` needs `value: number | undefined` to satisfy recharts strict typing
- `labelFormatter` needs `label: unknown` (not `string`) for recharts compatibility
---

## 2026-02-23 - V6-011
- **Implemented**: Risk Dashboard page — VaR/drawdown/correlation visualization
- **Files created**:
  - `frontend/src/pages/RiskDashboard.tsx` — VaR gauge cards (4 types), portfolio heat bar, drawdown curve chart, engine correlation heatmap, position exposure pie/bar, concentration monitor
  - `tests/dashboard/test_drawdown_api.py` — 10 tests: TestDrawdownEndpoint (no_manager, no_pm, with_history, history_values), TestPortfolioManagerDrawdownHistory (initial_empty, report_adds_entry, multiple_reports, capped_at_1000, zero_on_new_high, nonzero_after_loss)
- **Files modified**:
  - `src/bot/engines/portfolio_manager.py` — Added `_drawdown_history: list[dict]`, updated `report_pnl()` to append drawdown/equity entry, capped at 1000
  - `src/bot/dashboard/app.py` — Added `GET /api/risk/drawdown` endpoint on risk_router
  - `frontend/src/api/types.ts` — Added RiskPortfolioMetrics, DrawdownPoint, CorrelationReport interfaces
  - `frontend/src/App.tsx` — Added RiskDashboard import, /risk nav item, and Route
  - `scripts/ralph/prd.json` — Marked V6-011 passes: true
- **Tests**: 10 new, 2528 total passing (+ 3 pre-existing failures)
- **Learnings for future iterations:**
  - `Promise.allSettled()` is essential for dashboard pages that aggregate multiple API calls with graceful degradation
  - Recharts TypeScript strictness: formatter callbacks take `value | undefined`, labelFormatter takes `unknown`
  - RadialBarChart semicircle: `startAngle=180 endAngle=0` with `cx="50%" cy="80%"` for bottom alignment
  - AreaChart reversed Y-axis: `reversed` prop on YAxis for drawdown (higher=worse)
  - `datetime.now(timezone.utc)` for timestamp generation in drawdown history
---
